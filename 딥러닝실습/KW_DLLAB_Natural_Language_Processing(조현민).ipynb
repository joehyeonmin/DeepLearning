{"cells":[{"cell_type":"markdown","metadata":{"id":"b5evNZsjt2WF"},"source":["# Natural Language Processing\n","\n","by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n","/ [GitHub](https://github.com/Hvass-Labs/TensorFlow-Tutorials) / [Videos on YouTube](https://www.youtube.com/playlist?list=PL9Hr9sNUjfsmEu1ZniY0XpHSzl5uihcXZ)\n","\n","Modified by uramoon@kw.ac.kr"]},{"cell_type":"markdown","metadata":{"id":"UMtBnW6Vt2WJ"},"source":["## Introduction\n","\n","감성 분석 (Sentiment Analysis)은 자연어 처리 (Natural Language Processing, NLP)를 이용하여 텍스트의 감정을 파악하는 기술이다. 이 노트북에서는 영화 리뷰가 긍정적인지 부정적인지 분류할 것이다.\n","\n","\"This movie is not very good.\"을 보면 \"very good\"은 긍정적인 감정이지만 \"not\"이 있기 때문에 부정적인 감정으로 분류되어야 한다. 이러한 것을 어떻게 학습시킬 수 있을까?\n","\n","1. 인공 신경망은 숫자를 입력으로 받아들이는데 텍스트를 어떻게 수치 데이터로 변환할지 생각해봐야 한다.\n","2. 문서의 길이는 문서마다 다른데 크기가 각기 다른 입력 데이터를 인공 신경망에 어떻게 입력해야할지 생각해봐야 한다. "]},{"cell_type":"markdown","metadata":{"id":"JMoL_tZ7t2WJ"},"source":["## Flowchart\n","\n","1. Tokenizer를 이용하여 각 단어를 정수로 변환한다. 예) the: 1, and: 2, a: 3, ...\n","2. 임베딩 (embedding)을 통해 각 정수를 n차원 벡터로 변환한다. (가까운 단어는 가깝게 위치)\n","3. 문서를 순환 신경망 (recurrent neural network, RNN)에 입력하여 0 (부정적)부터 1 (긍적적) 사이의 실수를 출력하게 훈련한다.\n","\n","The flowchart of the algorithm is roughly:\n","\n","<img src=\"https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/20_natural_language_flowchart.png?raw=1\" alt=\"Flowchart NLP\" style=\"width: 300px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"JdGt4vEDt2WK"},"source":["## Recurrent Neural Network (RNN)\n","\n","RNN의 기본 유닛은 Recurrent Unit (RU)으로 LSTM (Long-Short-Term-Memory)과 성능 하락을 최소화하면서 LSTM을 단순화한 GRU (Gated Recurrent Unit)가 많이 사용된다.\n","\n","RU는 과거의 상태를 기억하고 있다가 현재의 입력에 대해 자신의 상태를 바꾸면서 값을 출력한다.\n","\n","새로운 상태는 과거의 상태와 현재의 입력에 따라 결정된다. 예를 들어 최근의 입력에 \"not\"이 있었고, 현재의 입력이 \"good\"이라면 새로운 상태는 \"not good\"에 해당하는 부정적인 감정을 기억할 것이다. \n","\n","![Recurrent unit](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/20_recurrent_unit.png?raw=1)"]},{"cell_type":"markdown","metadata":{"id":"CTHceb8_t2WK"},"source":["### Unrolled Network\n","\n","RNN에서 RU가 다음 입력에 과거의 정보를 전달하는 과정을 다음과 같이 펼쳐서 도식화할 수 있다. \n","\n","RU의 메모리는 0으로 초기화된다.\n","\n","가장 처음 \"this\"가 입력되면 메모리는 새로운 상태를 저장하고 무언가를 출력하지만 출력값을 사용하지는 않는다. 글을 끝까지 읽었을 때의 출력값만 활용할 것이다.\n","\n","두 번째 단어는 \"is\"인데 바로 전에 읽은 \"this\"와 결합하여 새로운 상태를 저장한다.\n","\n","세 번째 단어는 \"not\"인데 \"not\"을 기억하고 있다가 나중에 \"good\"을 읽었을 때 그것을 부정적인 단어로 바꿔주는 역할을 할 것이다. \n","\n","문서의 마지막 단어를 읽었을 때 0부터 1사이의 값을 출력하는데 이를 활용하여 감성 분석을 수행한다.\n","\n","Note that for the sake of clarity, this figure doesn't show the mapping from text-words to integer-tokens and embedding-vectors, as well as the fully-connected Sigmoid layer on the output.\n","\n","![Unrolled network](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/20_unrolled_flowchart.png?raw=1)"]},{"cell_type":"markdown","metadata":{"id":"PJ_a-tGlt2WL"},"source":["### 3-Layer Unrolled Network\n","\n","이 노트북은 세 개의 층에서 RU를 사용합니다. \n","\n","단어가 하나씩 입력될 때마다 첫 번째 층의 RU는 처리한 결과를 두 번째 층의 RU에 전달하고, 두 번째 층의 RU가 처리한 결과를 세 번째 층의 RU에 전달합니다. 모든 문장을 다 읽었을 때 세 번째 층에서 나온 결과를 Dense 층에 연결하여 0과 1 사이의 실수를 출력하게 합니다.\n","\n","Note that for the sake of clarity, the mapping of text-words to integer-tokens and embedding-vectors has been omitted from this figure.\n","\n","![Unrolled 3-layer network](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/20_unrolled_3layers_flowchart.png?raw=1)"]},{"cell_type":"markdown","metadata":{"id":"Ib5anMcIt2WL"},"source":["### Exploding & Vanishing Gradients\n","\n","매 스텝 새로운 단어가 들어올 때마다 내부 상태를 변경할 경우 기울기 소실 혹은 폭주 문제가 발생할 수 있습니다.\n","\n","하나의 텍스트가 500개의 단어로 구성된다고 했을 때 내부 상태가 500번 변화하게 되는데 각 변화마다 기울기를 곱할 때 기울기가 1 미만이면 그 값이 0에 가까워지고, 기울기가 1보다 크면 그 값은 매우 커집니다.\n","\n","이러한 기울기 소실 / 폭주 문제를 해결하기 위해 매 입력마다 상태가 변화하지 않는 능력을 지닌 장기기억 메모리를 도입한 것이 LSTM과 GRU입니다."]},{"cell_type":"markdown","source":["## 준비 단계\n","1. 런타임을 GPU로 설정해주세요.\n","2. imdb.py와 download.py 파일을 Colab 환경에 복사해주세요."],"metadata":{"id":"Pqee1-G7BMYi"}},{"cell_type":"markdown","metadata":{"id":"zwiNseb1t2WM"},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"NACTD0ZTt2WM","executionInfo":{"status":"ok","timestamp":1651818126105,"user_tz":-540,"elapsed":5659,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import numpy as np\n","from scipy.spatial.distance import cdist"]},{"cell_type":"markdown","metadata":{"id":"UsG8d_Kst2WN"},"source":["We need to import several things from Keras."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"1NoU7t1Ht2WO","executionInfo":{"status":"ok","timestamp":1651818130463,"user_tz":-540,"elapsed":365,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, GRU, Embedding, LSTM\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"markdown","metadata":{"id":"XiZrp2Ppt2WP"},"source":["## Load Data\n","\n","IMDB에서 50,000개의 영화 리뷰를 다운받습니다. Keras에 정제된 IMDB 데이터셋이 있지만 단어를 정수로 변환하는 작업을 직접 수행하기 위해 정제되지 않은 데이터셋을 사용합니다. 50,000개 중 25,000개는 훈련 데이터, 나머지 25,000개는 테스트 데이터입니다.\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"RZWc7HVCt2WP","executionInfo":{"status":"ok","timestamp":1651818231962,"user_tz":-540,"elapsed":345,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["import imdb"]},{"cell_type":"markdown","metadata":{"id":"xfwqtMAtt2WQ"},"source":["Automatically download and extract the files."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ShNhaNQ4t2WQ","outputId":"9af96c07-f05f-4604-958f-e42101cc07b8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651818311000,"user_tz":-540,"elapsed":73489,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["- Download progress: 100.0%\n","Download finished. Extracting files.\n","Done.\n"]}],"source":["imdb.maybe_download_and_extract()"]},{"cell_type":"markdown","source":["## 다운받은 파일 확인\n","폴더를 새로고침하면 다운로드 받은 data 폴더가 보입니다.\n","\n","1. 훈련 데이터 폴더: data/IMDB/aclImdb/train/ \n","2. 테스트 데이터 폴더: data/IMDB/aclImdb/test/ "],"metadata":{"id":"rBXbQObrB5lv"}},{"cell_type":"code","source":["# 훈련 데이터에서 긍정적 리뷰 확인 (10점)\n","#  This movie gets better each time I see it (which is quite often).\n","!cat data/IMDB/aclImdb/train/pos/10001_10.txt"],"metadata":{"id":"KctXlcN9B4Nn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651818316877,"user_tz":-540,"elapsed":341,"user":{"displayName":"조현민","userId":"13701119480392953941"}},"outputId":"c7924d07-3486-4d21-d78e-c21cbbb3134b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I'm a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often)."]}]},{"cell_type":"code","source":["# 훈련 데이터에서 부정적 리뷰 확인 (1점)\n","# this story is too painful to watch. \n","!cat data/IMDB/aclImdb/train/neg/10002_1.txt"],"metadata":{"id":"H0DdsDcDDDDO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651818324192,"user_tz":-540,"elapsed":328,"user":{"displayName":"조현민","userId":"13701119480392953941"}},"outputId":"0c112385-d292-49ba-fd1b-5620e5564482"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Sorry everyone,,, I know this is supposed to be an \"art\" film,, but wow, they should have handed out guns at the screening so people could blow their brains out and not watch. Although the scene design and photographic direction was excellent, this story is too painful to watch. The absence of a sound track was brutal. The loooonnnnng shots were too long. How long can you watch two people just sitting there and talking? Especially when the dialogue is two people complaining. I really had a hard time just getting through this film. The performances were excellent, but how much of that dark, sombre, uninspired, stuff can you take? The only thing i liked was Maureen Stapleton and her red dress and dancing scene. Otherwise this was a ripoff of Bergman. And i'm no fan f his either. I think anyone who says they enjoyed 1 1/2 hours of this is,, well, lying."]}]},{"cell_type":"markdown","metadata":{"id":"YI5HZG9ut2WQ"},"source":["## 훈련 데이터와 테스트 데이터 만들기\n","y 값에는 0 (부정)과 1 (긍정)이 기록됩니다.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"anIzfJ5-t2WQ","executionInfo":{"status":"ok","timestamp":1651818332349,"user_tz":-540,"elapsed":2483,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["x_train_text, y_train = imdb.load_data(train=True)\n","x_test_text, y_test = imdb.load_data(train=False)\n","\n","# Convert to numpy arrays.\n","y_train = np.array(y_train)\n","y_test = np.array(y_test)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"N-fgoTa9t2WR","outputId":"091f0368-7d0b-4146-a6fa-66146f37666c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651818335412,"user_tz":-540,"elapsed":316,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Train-set size:  25000\n","Test-set size:   25000\n"]}],"source":["print(\"Train-set size: \", len(x_train_text))\n","print(\"Test-set size:  \", len(x_test_text))"]},{"cell_type":"code","source":["# TODO: 훈련데이터의 0번째 x값(리뷰)과 y값(정답)을 출력해보세요.\n","print(x_train_text[0])\n","print(y_train[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xI6MDlNIEVng","executionInfo":{"status":"ok","timestamp":1651818338941,"user_tz":-540,"elapsed":317,"user":{"displayName":"조현민","userId":"13701119480392953941"}},"outputId":"b5bc5caf-27db-4fb5-fc2a-b8f1cc1f8b3c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Some spoilers If you are a big horror movie fan, then you will know that Halloween paved the way for many slasher films. Often imitated, never duplicated, this movie is a true horror classic and is definitely one of the scariest movies ever made, if not THE scariest.<br /><br />I actually saw this movie after seeing the rest of the series (don't ask me why). I honestly saw the other 7 movies before seeing this one, that's just how it worked out. But I would have to say this one blows all the others away. It is genuinely frightening, and seeing Michael pop out from behind a bush or walk around in the dark sends a chill down your spine. My favorite part of the movie was when Michael stabs a guy, and leans his head to one side. It is one of the eeriest images in movie history.<br /><br />Later slashers, such as the Friday the 13th films, were more fun and less intense than this movie. I do like the F13 series better than the Halloween series, but this movie alone is better than all the F13 movies. Michael Myers is such a scary villain because he is realistic, you could imagine a crazed guy like him going around killing people. I admit this movie gave me nightmares after watching it for a few nights.<br /><br />What's great about this movie is that it doesn't rely on gore or humor to entertain the audience. It is just pure terror. It's too bad the later films of the series swayed from this one, because this is as good of an example of a spectacular slasher movie as they come.\n","1.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"iAD8-TAWt2WS"},"source":["## Tokenizer\n","\n","인공신경망은 실수값들만 처리할 수 있기 때문에 우선 tokenizer를 통해 각 단어를 정수로 변환하는 과정을 거칩니다. Tokenizer는 데이터셋에서 가장 많이 등장하는 n개의 단어를 정수로 변환합니다. 예) the: 1, and: 2, a: 3, ... \n","<br>\n","일단 10000개의 단어만 사용하여 모델을 만들어봅시다."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"UZtN0bXxt2WS","executionInfo":{"status":"ok","timestamp":1651818376464,"user_tz":-540,"elapsed":339,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["num_words = 10000\n","tokenizer = Tokenizer(num_words=num_words)"]},{"cell_type":"markdown","metadata":{"id":"rEZUMyLIt2WS"},"source":["훈련 데이터와 테스트 데이터에 들어 있는 모든 단어를 변환할 필요가 있기 때문에 훈련 데이터와 테스트 데이터를 함께 tokenizer에 입력합니다."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"5qXkL1ONt2WS","outputId":"b4b73d56-ad81-4b04-9007-73690cdf09c1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651818391420,"user_tz":-540,"elapsed":10042,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 9.69 s, sys: 65.8 ms, total: 9.75 s\n","Wall time: 9.74 s\n"]}],"source":["%%time\n","data_text = x_train_text + x_test_text\n","tokenizer.fit_on_texts(data_text)"]},{"cell_type":"markdown","metadata":{"id":"DvJr9Xfyt2WT"},"source":["Tokenizer가 찾아낸 많이 등장하는 단어들입니다. 해당 단어들은 정수로 변환되고 나머지 단어들은 제거될 것입니다."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"ZU5AyhO8t2WT","outputId":"2b793aa6-5231-4106-9619-d3d233111e18","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651818406770,"user_tz":-540,"elapsed":366,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'the': 1,\n"," 'and': 2,\n"," 'a': 3,\n"," 'of': 4,\n"," 'to': 5,\n"," 'is': 6,\n"," 'br': 7,\n"," 'in': 8,\n"," 'it': 9,\n"," 'i': 10,\n"," 'this': 11,\n"," 'that': 12,\n"," 'was': 13,\n"," 'as': 14,\n"," 'for': 15,\n"," 'with': 16,\n"," 'movie': 17,\n"," 'but': 18,\n"," 'film': 19,\n"," 'on': 20,\n"," 'not': 21,\n"," 'you': 22,\n"," 'are': 23,\n"," 'his': 24,\n"," 'have': 25,\n"," 'be': 26,\n"," 'one': 27,\n"," 'he': 28,\n"," 'all': 29,\n"," 'at': 30,\n"," 'by': 31,\n"," 'an': 32,\n"," 'they': 33,\n"," 'so': 34,\n"," 'who': 35,\n"," 'from': 36,\n"," 'like': 37,\n"," 'or': 38,\n"," 'just': 39,\n"," 'her': 40,\n"," 'out': 41,\n"," 'about': 42,\n"," 'if': 43,\n"," \"it's\": 44,\n"," 'has': 45,\n"," 'there': 46,\n"," 'some': 47,\n"," 'what': 48,\n"," 'good': 49,\n"," 'when': 50,\n"," 'more': 51,\n"," 'very': 52,\n"," 'up': 53,\n"," 'no': 54,\n"," 'time': 55,\n"," 'my': 56,\n"," 'even': 57,\n"," 'would': 58,\n"," 'she': 59,\n"," 'which': 60,\n"," 'only': 61,\n"," 'really': 62,\n"," 'see': 63,\n"," 'story': 64,\n"," 'their': 65,\n"," 'had': 66,\n"," 'can': 67,\n"," 'me': 68,\n"," 'well': 69,\n"," 'were': 70,\n"," 'than': 71,\n"," 'much': 72,\n"," 'we': 73,\n"," 'bad': 74,\n"," 'been': 75,\n"," 'get': 76,\n"," 'do': 77,\n"," 'great': 78,\n"," 'other': 79,\n"," 'will': 80,\n"," 'also': 81,\n"," 'into': 82,\n"," 'people': 83,\n"," 'because': 84,\n"," 'how': 85,\n"," 'first': 86,\n"," 'him': 87,\n"," 'most': 88,\n"," \"don't\": 89,\n"," 'made': 90,\n"," 'then': 91,\n"," 'its': 92,\n"," 'them': 93,\n"," 'make': 94,\n"," 'way': 95,\n"," 'too': 96,\n"," 'movies': 97,\n"," 'could': 98,\n"," 'any': 99,\n"," 'after': 100,\n"," 'think': 101,\n"," 'characters': 102,\n"," 'watch': 103,\n"," 'films': 104,\n"," 'two': 105,\n"," 'many': 106,\n"," 'seen': 107,\n"," 'character': 108,\n"," 'being': 109,\n"," 'never': 110,\n"," 'plot': 111,\n"," 'love': 112,\n"," 'acting': 113,\n"," 'life': 114,\n"," 'did': 115,\n"," 'best': 116,\n"," 'where': 117,\n"," 'know': 118,\n"," 'show': 119,\n"," 'little': 120,\n"," 'over': 121,\n"," 'off': 122,\n"," 'ever': 123,\n"," 'does': 124,\n"," 'your': 125,\n"," 'better': 126,\n"," 'end': 127,\n"," 'man': 128,\n"," 'scene': 129,\n"," 'still': 130,\n"," 'say': 131,\n"," 'these': 132,\n"," 'here': 133,\n"," 'why': 134,\n"," 'scenes': 135,\n"," 'while': 136,\n"," 'something': 137,\n"," 'such': 138,\n"," 'go': 139,\n"," 'through': 140,\n"," 'back': 141,\n"," 'should': 142,\n"," 'those': 143,\n"," 'real': 144,\n"," \"i'm\": 145,\n"," 'now': 146,\n"," 'watching': 147,\n"," 'thing': 148,\n"," \"doesn't\": 149,\n"," 'actors': 150,\n"," 'though': 151,\n"," 'funny': 152,\n"," 'years': 153,\n"," \"didn't\": 154,\n"," 'old': 155,\n"," 'another': 156,\n"," '10': 157,\n"," 'work': 158,\n"," 'before': 159,\n"," 'actually': 160,\n"," 'nothing': 161,\n"," 'makes': 162,\n"," 'look': 163,\n"," 'director': 164,\n"," 'find': 165,\n"," 'going': 166,\n"," 'same': 167,\n"," 'new': 168,\n"," 'lot': 169,\n"," 'every': 170,\n"," 'few': 171,\n"," 'again': 172,\n"," 'part': 173,\n"," 'cast': 174,\n"," 'down': 175,\n"," 'us': 176,\n"," 'things': 177,\n"," 'want': 178,\n"," 'quite': 179,\n"," 'pretty': 180,\n"," 'world': 181,\n"," 'horror': 182,\n"," 'around': 183,\n"," 'seems': 184,\n"," \"can't\": 185,\n"," 'young': 186,\n"," 'take': 187,\n"," 'however': 188,\n"," 'got': 189,\n"," 'thought': 190,\n"," 'big': 191,\n"," 'fact': 192,\n"," 'enough': 193,\n"," 'long': 194,\n"," 'both': 195,\n"," \"that's\": 196,\n"," 'give': 197,\n"," \"i've\": 198,\n"," 'own': 199,\n"," 'may': 200,\n"," 'between': 201,\n"," 'comedy': 202,\n"," 'right': 203,\n"," 'series': 204,\n"," 'action': 205,\n"," 'must': 206,\n"," 'music': 207,\n"," 'without': 208,\n"," 'times': 209,\n"," 'saw': 210,\n"," 'always': 211,\n"," 'original': 212,\n"," \"isn't\": 213,\n"," 'role': 214,\n"," 'come': 215,\n"," 'almost': 216,\n"," 'gets': 217,\n"," 'interesting': 218,\n"," 'guy': 219,\n"," 'point': 220,\n"," 'done': 221,\n"," \"there's\": 222,\n"," 'whole': 223,\n"," 'least': 224,\n"," 'far': 225,\n"," 'bit': 226,\n"," 'script': 227,\n"," 'minutes': 228,\n"," 'feel': 229,\n"," '2': 230,\n"," 'anything': 231,\n"," 'making': 232,\n"," 'might': 233,\n"," 'since': 234,\n"," 'am': 235,\n"," 'family': 236,\n"," \"he's\": 237,\n"," 'last': 238,\n"," 'probably': 239,\n"," 'tv': 240,\n"," 'performance': 241,\n"," 'kind': 242,\n"," 'away': 243,\n"," 'yet': 244,\n"," 'fun': 245,\n"," 'worst': 246,\n"," 'sure': 247,\n"," 'rather': 248,\n"," 'hard': 249,\n"," 'anyone': 250,\n"," 'girl': 251,\n"," 'each': 252,\n"," 'played': 253,\n"," 'day': 254,\n"," 'found': 255,\n"," 'looking': 256,\n"," 'woman': 257,\n"," 'screen': 258,\n"," 'although': 259,\n"," 'our': 260,\n"," 'especially': 261,\n"," 'believe': 262,\n"," 'having': 263,\n"," 'trying': 264,\n"," 'course': 265,\n"," 'dvd': 266,\n"," 'everything': 267,\n"," 'set': 268,\n"," 'goes': 269,\n"," 'comes': 270,\n"," 'put': 271,\n"," 'ending': 272,\n"," 'maybe': 273,\n"," 'place': 274,\n"," 'book': 275,\n"," 'shows': 276,\n"," 'three': 277,\n"," 'worth': 278,\n"," 'different': 279,\n"," 'main': 280,\n"," 'once': 281,\n"," 'sense': 282,\n"," 'american': 283,\n"," 'reason': 284,\n"," 'looks': 285,\n"," 'effects': 286,\n"," 'watched': 287,\n"," 'play': 288,\n"," 'true': 289,\n"," 'money': 290,\n"," 'actor': 291,\n"," \"wasn't\": 292,\n"," 'job': 293,\n"," 'together': 294,\n"," 'war': 295,\n"," 'someone': 296,\n"," 'plays': 297,\n"," 'instead': 298,\n"," 'high': 299,\n"," 'during': 300,\n"," 'said': 301,\n"," 'year': 302,\n"," 'half': 303,\n"," 'everyone': 304,\n"," 'later': 305,\n"," 'takes': 306,\n"," '1': 307,\n"," 'seem': 308,\n"," 'audience': 309,\n"," 'special': 310,\n"," 'beautiful': 311,\n"," 'left': 312,\n"," 'himself': 313,\n"," 'seeing': 314,\n"," 'john': 315,\n"," 'night': 316,\n"," 'black': 317,\n"," 'version': 318,\n"," 'shot': 319,\n"," 'excellent': 320,\n"," 'idea': 321,\n"," 'house': 322,\n"," 'mind': 323,\n"," 'star': 324,\n"," 'wife': 325,\n"," 'fan': 326,\n"," 'death': 327,\n"," 'used': 328,\n"," 'else': 329,\n"," 'simply': 330,\n"," 'nice': 331,\n"," 'budget': 332,\n"," 'poor': 333,\n"," 'completely': 334,\n"," 'short': 335,\n"," 'second': 336,\n"," \"you're\": 337,\n"," '3': 338,\n"," 'read': 339,\n"," 'less': 340,\n"," 'along': 341,\n"," 'top': 342,\n"," 'help': 343,\n"," 'home': 344,\n"," 'men': 345,\n"," 'either': 346,\n"," 'line': 347,\n"," 'boring': 348,\n"," 'dead': 349,\n"," 'friends': 350,\n"," 'kids': 351,\n"," 'try': 352,\n"," 'production': 353,\n"," 'enjoy': 354,\n"," 'camera': 355,\n"," 'wrong': 356,\n"," 'use': 357,\n"," 'given': 358,\n"," 'low': 359,\n"," 'classic': 360,\n"," 'father': 361,\n"," 'need': 362,\n"," 'full': 363,\n"," 'stupid': 364,\n"," 'until': 365,\n"," 'next': 366,\n"," 'performances': 367,\n"," 'school': 368,\n"," 'hollywood': 369,\n"," 'rest': 370,\n"," 'truly': 371,\n"," 'awful': 372,\n"," 'video': 373,\n"," 'couple': 374,\n"," 'start': 375,\n"," 'sex': 376,\n"," 'recommend': 377,\n"," 'women': 378,\n"," 'let': 379,\n"," 'tell': 380,\n"," 'terrible': 381,\n"," 'remember': 382,\n"," 'mean': 383,\n"," 'came': 384,\n"," 'understand': 385,\n"," 'getting': 386,\n"," 'perhaps': 387,\n"," 'moments': 388,\n"," 'name': 389,\n"," 'keep': 390,\n"," 'face': 391,\n"," 'itself': 392,\n"," 'wonderful': 393,\n"," 'playing': 394,\n"," 'human': 395,\n"," 'style': 396,\n"," 'small': 397,\n"," 'episode': 398,\n"," 'perfect': 399,\n"," 'others': 400,\n"," 'person': 401,\n"," 'doing': 402,\n"," 'often': 403,\n"," 'early': 404,\n"," 'stars': 405,\n"," 'definitely': 406,\n"," 'written': 407,\n"," 'head': 408,\n"," 'lines': 409,\n"," 'dialogue': 410,\n"," 'gives': 411,\n"," 'piece': 412,\n"," \"couldn't\": 413,\n"," 'went': 414,\n"," 'finally': 415,\n"," 'mother': 416,\n"," 'title': 417,\n"," 'case': 418,\n"," 'absolutely': 419,\n"," 'live': 420,\n"," 'boy': 421,\n"," 'yes': 422,\n"," 'laugh': 423,\n"," 'certainly': 424,\n"," 'liked': 425,\n"," 'become': 426,\n"," 'entertaining': 427,\n"," 'worse': 428,\n"," 'oh': 429,\n"," 'sort': 430,\n"," 'loved': 431,\n"," 'lost': 432,\n"," 'called': 433,\n"," 'hope': 434,\n"," 'picture': 435,\n"," 'felt': 436,\n"," 'overall': 437,\n"," 'entire': 438,\n"," 'mr': 439,\n"," 'several': 440,\n"," 'based': 441,\n"," 'supposed': 442,\n"," 'cinema': 443,\n"," 'friend': 444,\n"," 'guys': 445,\n"," 'sound': 446,\n"," '5': 447,\n"," 'problem': 448,\n"," 'drama': 449,\n"," 'against': 450,\n"," 'waste': 451,\n"," 'white': 452,\n"," 'beginning': 453,\n"," '4': 454,\n"," 'fans': 455,\n"," 'totally': 456,\n"," 'dark': 457,\n"," 'care': 458,\n"," 'direction': 459,\n"," 'humor': 460,\n"," 'wanted': 461,\n"," \"she's\": 462,\n"," 'seemed': 463,\n"," 'game': 464,\n"," 'under': 465,\n"," 'children': 466,\n"," 'despite': 467,\n"," 'lives': 468,\n"," 'lead': 469,\n"," 'guess': 470,\n"," 'example': 471,\n"," 'already': 472,\n"," 'final': 473,\n"," 'throughout': 474,\n"," \"you'll\": 475,\n"," 'evil': 476,\n"," 'turn': 477,\n"," 'becomes': 478,\n"," 'unfortunately': 479,\n"," 'able': 480,\n"," 'quality': 481,\n"," \"i'd\": 482,\n"," 'days': 483,\n"," 'history': 484,\n"," 'fine': 485,\n"," 'side': 486,\n"," 'wants': 487,\n"," 'heart': 488,\n"," 'horrible': 489,\n"," 'writing': 490,\n"," 'amazing': 491,\n"," 'b': 492,\n"," 'flick': 493,\n"," 'killer': 494,\n"," 'run': 495,\n"," 'son': 496,\n"," '\\x96': 497,\n"," 'michael': 498,\n"," 'works': 499,\n"," 'close': 500,\n"," \"they're\": 501,\n"," 'act': 502,\n"," 'art': 503,\n"," 'matter': 504,\n"," 'kill': 505,\n"," 'etc': 506,\n"," 'tries': 507,\n"," \"won't\": 508,\n"," 'past': 509,\n"," 'town': 510,\n"," 'enjoyed': 511,\n"," 'turns': 512,\n"," 'brilliant': 513,\n"," 'gave': 514,\n"," 'behind': 515,\n"," 'parts': 516,\n"," 'stuff': 517,\n"," 'genre': 518,\n"," 'eyes': 519,\n"," 'car': 520,\n"," 'favorite': 521,\n"," 'directed': 522,\n"," 'late': 523,\n"," 'hand': 524,\n"," 'expect': 525,\n"," 'soon': 526,\n"," 'hour': 527,\n"," 'obviously': 528,\n"," 'themselves': 529,\n"," 'sometimes': 530,\n"," 'killed': 531,\n"," 'actress': 532,\n"," 'thinking': 533,\n"," 'girls': 534,\n"," 'child': 535,\n"," 'viewer': 536,\n"," 'starts': 537,\n"," 'city': 538,\n"," 'myself': 539,\n"," 'decent': 540,\n"," 'highly': 541,\n"," 'stop': 542,\n"," 'type': 543,\n"," 'self': 544,\n"," 'god': 545,\n"," 'says': 546,\n"," 'group': 547,\n"," 'anyway': 548,\n"," 'voice': 549,\n"," 'took': 550,\n"," 'known': 551,\n"," 'blood': 552,\n"," 'kid': 553,\n"," 'heard': 554,\n"," 'happens': 555,\n"," 'except': 556,\n"," 'fight': 557,\n"," 'feeling': 558,\n"," 'experience': 559,\n"," 'coming': 560,\n"," 'slow': 561,\n"," 'daughter': 562,\n"," 'writer': 563,\n"," 'stories': 564,\n"," 'moment': 565,\n"," 'told': 566,\n"," 'leave': 567,\n"," 'extremely': 568,\n"," 'score': 569,\n"," 'violence': 570,\n"," 'involved': 571,\n"," 'police': 572,\n"," 'strong': 573,\n"," 'chance': 574,\n"," 'lack': 575,\n"," 'cannot': 576,\n"," 'hit': 577,\n"," 'hilarious': 578,\n"," 'roles': 579,\n"," 's': 580,\n"," 'happen': 581,\n"," 'wonder': 582,\n"," 'particularly': 583,\n"," 'ok': 584,\n"," 'including': 585,\n"," 'living': 586,\n"," 'save': 587,\n"," 'looked': 588,\n"," \"wouldn't\": 589,\n"," 'crap': 590,\n"," 'simple': 591,\n"," 'please': 592,\n"," 'murder': 593,\n"," 'cool': 594,\n"," 'obvious': 595,\n"," 'happened': 596,\n"," 'complete': 597,\n"," 'cut': 598,\n"," 'age': 599,\n"," 'serious': 600,\n"," 'gore': 601,\n"," 'attempt': 602,\n"," 'hell': 603,\n"," 'ago': 604,\n"," 'song': 605,\n"," 'shown': 606,\n"," 'taken': 607,\n"," 'english': 608,\n"," 'james': 609,\n"," 'robert': 610,\n"," 'david': 611,\n"," 'seriously': 612,\n"," 'released': 613,\n"," 'reality': 614,\n"," 'opening': 615,\n"," 'interest': 616,\n"," 'jokes': 617,\n"," 'across': 618,\n"," 'none': 619,\n"," 'hero': 620,\n"," 'today': 621,\n"," 'possible': 622,\n"," 'exactly': 623,\n"," 'alone': 624,\n"," 'sad': 625,\n"," 'brother': 626,\n"," 'number': 627,\n"," 'saying': 628,\n"," 'career': 629,\n"," \"film's\": 630,\n"," 'hours': 631,\n"," 'usually': 632,\n"," 'cinematography': 633,\n"," 'talent': 634,\n"," 'view': 635,\n"," 'running': 636,\n"," 'yourself': 637,\n"," 'annoying': 638,\n"," 'relationship': 639,\n"," 'documentary': 640,\n"," 'wish': 641,\n"," 'huge': 642,\n"," 'order': 643,\n"," 'shots': 644,\n"," 'whose': 645,\n"," 'ridiculous': 646,\n"," 'taking': 647,\n"," 'important': 648,\n"," 'light': 649,\n"," 'body': 650,\n"," 'middle': 651,\n"," 'level': 652,\n"," 'ends': 653,\n"," 'call': 654,\n"," 'started': 655,\n"," 'female': 656,\n"," \"i'll\": 657,\n"," 'husband': 658,\n"," 'four': 659,\n"," 'power': 660,\n"," 'turned': 661,\n"," 'major': 662,\n"," 'word': 663,\n"," 'opinion': 664,\n"," 'change': 665,\n"," 'mostly': 666,\n"," 'usual': 667,\n"," 'scary': 668,\n"," 'silly': 669,\n"," 'rating': 670,\n"," 'beyond': 671,\n"," 'somewhat': 672,\n"," 'happy': 673,\n"," 'ones': 674,\n"," 'words': 675,\n"," 'room': 676,\n"," 'knew': 677,\n"," 'knows': 678,\n"," 'country': 679,\n"," 'disappointed': 680,\n"," 'talking': 681,\n"," 'novel': 682,\n"," 'apparently': 683,\n"," 'non': 684,\n"," 'strange': 685,\n"," 'attention': 686,\n"," 'upon': 687,\n"," 'finds': 688,\n"," 'basically': 689,\n"," 'single': 690,\n"," 'cheap': 691,\n"," 'modern': 692,\n"," 'due': 693,\n"," 'jack': 694,\n"," 'television': 695,\n"," 'musical': 696,\n"," 'problems': 697,\n"," 'miss': 698,\n"," 'episodes': 699,\n"," 'clearly': 700,\n"," 'local': 701,\n"," '7': 702,\n"," 'british': 703,\n"," 'thriller': 704,\n"," 'talk': 705,\n"," 'events': 706,\n"," 'five': 707,\n"," 'sequence': 708,\n"," \"aren't\": 709,\n"," 'class': 710,\n"," 'french': 711,\n"," 'moving': 712,\n"," 'ten': 713,\n"," 'fast': 714,\n"," 'earth': 715,\n"," 'review': 716,\n"," 'tells': 717,\n"," 'predictable': 718,\n"," 'songs': 719,\n"," 'team': 720,\n"," 'comic': 721,\n"," 'straight': 722,\n"," '8': 723,\n"," 'whether': 724,\n"," 'die': 725,\n"," 'add': 726,\n"," 'dialog': 727,\n"," 'entertainment': 728,\n"," 'above': 729,\n"," 'sets': 730,\n"," 'future': 731,\n"," 'enjoyable': 732,\n"," 'appears': 733,\n"," 'near': 734,\n"," 'space': 735,\n"," 'easily': 736,\n"," 'hate': 737,\n"," 'soundtrack': 738,\n"," 'bring': 739,\n"," 'giving': 740,\n"," 'lots': 741,\n"," 'similar': 742,\n"," 'romantic': 743,\n"," 'george': 744,\n"," 'supporting': 745,\n"," 'release': 746,\n"," 'mention': 747,\n"," 'within': 748,\n"," 'filmed': 749,\n"," 'message': 750,\n"," 'sequel': 751,\n"," 'clear': 752,\n"," 'falls': 753,\n"," 'needs': 754,\n"," \"haven't\": 755,\n"," 'dull': 756,\n"," 'suspense': 757,\n"," 'eye': 758,\n"," 'bunch': 759,\n"," 'surprised': 760,\n"," 'showing': 761,\n"," 'tried': 762,\n"," 'sorry': 763,\n"," 'certain': 764,\n"," 'easy': 765,\n"," 'working': 766,\n"," 'ways': 767,\n"," 'theme': 768,\n"," 'theater': 769,\n"," 'named': 770,\n"," 'among': 771,\n"," \"what's\": 772,\n"," 'storyline': 773,\n"," 'monster': 774,\n"," 'king': 775,\n"," 'stay': 776,\n"," 'effort': 777,\n"," 'stand': 778,\n"," 'fall': 779,\n"," 'minute': 780,\n"," 'gone': 781,\n"," 'rock': 782,\n"," 'using': 783,\n"," '9': 784,\n"," 'feature': 785,\n"," 'comments': 786,\n"," 'buy': 787,\n"," \"'\": 788,\n"," 'typical': 789,\n"," 't': 790,\n"," 'sister': 791,\n"," 'editing': 792,\n"," 'tale': 793,\n"," 'avoid': 794,\n"," 'deal': 795,\n"," 'dr': 796,\n"," 'mystery': 797,\n"," 'doubt': 798,\n"," 'fantastic': 799,\n"," 'kept': 800,\n"," 'nearly': 801,\n"," 'okay': 802,\n"," 'feels': 803,\n"," 'subject': 804,\n"," 'viewing': 805,\n"," 'elements': 806,\n"," 'oscar': 807,\n"," 'check': 808,\n"," 'realistic': 809,\n"," 'points': 810,\n"," 'greatest': 811,\n"," 'means': 812,\n"," 'herself': 813,\n"," 'parents': 814,\n"," 'famous': 815,\n"," 'imagine': 816,\n"," 'rent': 817,\n"," 'viewers': 818,\n"," 'richard': 819,\n"," 'crime': 820,\n"," 'form': 821,\n"," 'peter': 822,\n"," 'actual': 823,\n"," 'lady': 824,\n"," 'general': 825,\n"," 'dog': 826,\n"," 'follow': 827,\n"," 'believable': 828,\n"," 'period': 829,\n"," 'red': 830,\n"," 'brought': 831,\n"," 'move': 832,\n"," 'material': 833,\n"," 'forget': 834,\n"," 'somehow': 835,\n"," 'begins': 836,\n"," 're': 837,\n"," 'reviews': 838,\n"," 'animation': 839,\n"," 'paul': 840,\n"," \"you've\": 841,\n"," 'leads': 842,\n"," 'weak': 843,\n"," 'figure': 844,\n"," 'surprise': 845,\n"," 'sit': 846,\n"," 'hear': 847,\n"," 'average': 848,\n"," 'open': 849,\n"," 'sequences': 850,\n"," 'killing': 851,\n"," 'atmosphere': 852,\n"," 'eventually': 853,\n"," 'learn': 854,\n"," 'tom': 855,\n"," 'premise': 856,\n"," 'wait': 857,\n"," '20': 858,\n"," 'sci': 859,\n"," 'deep': 860,\n"," 'fi': 861,\n"," 'expected': 862,\n"," 'whatever': 863,\n"," 'indeed': 864,\n"," 'note': 865,\n"," 'poorly': 866,\n"," 'particular': 867,\n"," 'lame': 868,\n"," 'dance': 869,\n"," 'imdb': 870,\n"," 'situation': 871,\n"," 'shame': 872,\n"," 'third': 873,\n"," 'york': 874,\n"," 'box': 875,\n"," 'truth': 876,\n"," 'decided': 877,\n"," 'free': 878,\n"," 'hot': 879,\n"," \"who's\": 880,\n"," 'difficult': 881,\n"," 'needed': 882,\n"," 'season': 883,\n"," 'acted': 884,\n"," 'leaves': 885,\n"," 'unless': 886,\n"," 'possibly': 887,\n"," 'emotional': 888,\n"," 'romance': 889,\n"," 'sexual': 890,\n"," 'gay': 891,\n"," 'boys': 892,\n"," 'footage': 893,\n"," 'write': 894,\n"," 'western': 895,\n"," 'forced': 896,\n"," 'credits': 897,\n"," 'became': 898,\n"," 'memorable': 899,\n"," 'doctor': 900,\n"," 'reading': 901,\n"," 'otherwise': 902,\n"," 'air': 903,\n"," 'de': 904,\n"," 'begin': 905,\n"," 'crew': 906,\n"," 'question': 907,\n"," 'society': 908,\n"," 'meet': 909,\n"," 'male': 910,\n"," \"let's\": 911,\n"," 'meets': 912,\n"," 'plus': 913,\n"," 'cheesy': 914,\n"," 'hands': 915,\n"," 'superb': 916,\n"," 'screenplay': 917,\n"," 'beauty': 918,\n"," 'interested': 919,\n"," 'street': 920,\n"," 'features': 921,\n"," 'perfectly': 922,\n"," 'masterpiece': 923,\n"," 'whom': 924,\n"," 'laughs': 925,\n"," 'stage': 926,\n"," 'nature': 927,\n"," 'effect': 928,\n"," 'comment': 929,\n"," 'forward': 930,\n"," 'nor': 931,\n"," 'previous': 932,\n"," 'e': 933,\n"," 'sounds': 934,\n"," 'badly': 935,\n"," 'japanese': 936,\n"," 'weird': 937,\n"," 'island': 938,\n"," 'personal': 939,\n"," 'inside': 940,\n"," 'quickly': 941,\n"," 'total': 942,\n"," 'keeps': 943,\n"," 'towards': 944,\n"," 'america': 945,\n"," 'result': 946,\n"," 'battle': 947,\n"," 'crazy': 948,\n"," 'worked': 949,\n"," 'setting': 950,\n"," 'incredibly': 951,\n"," 'earlier': 952,\n"," 'background': 953,\n"," 'mess': 954,\n"," 'cop': 955,\n"," 'writers': 956,\n"," 'fire': 957,\n"," 'copy': 958,\n"," 'unique': 959,\n"," 'dumb': 960,\n"," 'realize': 961,\n"," 'powerful': 962,\n"," 'lee': 963,\n"," 'mark': 964,\n"," 'business': 965,\n"," 'rate': 966,\n"," 'dramatic': 967,\n"," 'older': 968,\n"," 'pay': 969,\n"," 'following': 970,\n"," 'directors': 971,\n"," 'girlfriend': 972,\n"," 'joke': 973,\n"," 'plenty': 974,\n"," 'directing': 975,\n"," 'various': 976,\n"," 'baby': 977,\n"," 'creepy': 978,\n"," 'appear': 979,\n"," 'development': 980,\n"," 'brings': 981,\n"," 'front': 982,\n"," 'ask': 983,\n"," 'dream': 984,\n"," 'water': 985,\n"," 'admit': 986,\n"," 'bill': 987,\n"," 'rich': 988,\n"," 'apart': 989,\n"," 'joe': 990,\n"," 'political': 991,\n"," 'fairly': 992,\n"," 'reasons': 993,\n"," 'leading': 994,\n"," 'portrayed': 995,\n"," 'spent': 996,\n"," 'telling': 997,\n"," 'cover': 998,\n"," 'outside': 999,\n"," 'present': 1000,\n"," ...}"]},"metadata":{},"execution_count":15}],"source":["tokenizer.word_index"]},{"cell_type":"markdown","metadata":{"id":"_ZGGeitPt2WT"},"source":["Tokenizer로 훈련 데이터의 텍스트를 정수로 변환합니다."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"sOOcMmI3t2WT","executionInfo":{"status":"ok","timestamp":1651818419899,"user_tz":-540,"elapsed":3977,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["x_train_tokens = tokenizer.texts_to_sequences(x_train_text)"]},{"cell_type":"markdown","metadata":{"id":"VSI5KoKwt2WT"},"source":["For example, here is a text from the training-set:"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"bD7v-6HVt2WU","colab":{"base_uri":"https://localhost:8080/","height":167},"executionInfo":{"status":"ok","timestamp":1651818426192,"user_tz":-540,"elapsed":331,"user":{"displayName":"조현민","userId":"13701119480392953941"}},"outputId":"3693faa4-982a-4199-b999-cd1908e2ae54"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Some spoilers If you are a big horror movie fan, then you will know that Halloween paved the way for many slasher films. Often imitated, never duplicated, this movie is a true horror classic and is definitely one of the scariest movies ever made, if not THE scariest.<br /><br />I actually saw this movie after seeing the rest of the series (don't ask me why). I honestly saw the other 7 movies before seeing this one, that's just how it worked out. But I would have to say this one blows all the others away. It is genuinely frightening, and seeing Michael pop out from behind a bush or walk around in the dark sends a chill down your spine. My favorite part of the movie was when Michael stabs a guy, and leans his head to one side. It is one of the eeriest images in movie history.<br /><br />Later slashers, such as the Friday the 13th films, were more fun and less intense than this movie. I do like the F13 series better than the Halloween series, but this movie alone is better than all the F13 movies. Michael Myers is such a scary villain because he is realistic, you could imagine a crazed guy like him going around killing people. I admit this movie gave me nightmares after watching it for a few nights.<br /><br />What's great about this movie is that it doesn't rely on gore or humor to entertain the audience. It is just pure terror. It's too bad the later films of the series swayed from this one, because this is as good of an example of a spectacular slasher movie as they come.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}],"source":["# TODO: 훈련 데이터의 0번째 텍스트만 출력해보기\n","x_train_text[0]"]},{"cell_type":"markdown","metadata":{"id":"sX8tA1JQt2WU"},"source":["This text corresponds to the following list of tokens:"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"xVyrza1ct2WU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651818438075,"user_tz":-540,"elapsed":338,"user":{"displayName":"조현민","userId":"13701119480392953941"}},"outputId":"64dd7e85-2870-47d1-d1c7-37d3e0f1ee11"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  47, 1038,   43,   22,   23,    3,  191,  182,   17,  326,   91,\n","         22,   80,  118,   12, 2145,    1,   95,   15,  106, 1254,  104,\n","        403,  110,   11,   17,    6,    3,  289,  182,  360,    2,    6,\n","        406,   27,    4,    1, 6378,   97,  123,   90,   43,   21,    1,\n","       6378,    7,    7,   10,  160,  210,   11,   17,  100,  314,    1,\n","        370,    4,    1,  204,   89,  983,   68,  134,   10, 1228,  210,\n","          1,   79,  702,   97,  159,  314,   11,   27,  196,   39,   85,\n","          9,  949,   41,   18,   10,   58,   25,    5,  131,   11,   27,\n","       3303,   29,    1,  400,  243,    9,    6, 2039, 2611,    2,  314,\n","        498, 1560,   41,   36,  515,    3, 3599,   38, 1192,  183,    8,\n","          1,  457, 3249,    3, 7540,  175,  125, 6340,   56,  521,  173,\n","          4,    1,   17,   13,   50,  498, 9597,    3,  219,    2,   24,\n","        408,    5,   27,  486,    9,    6,   27,    4,    1, 1132,    8,\n","         17,  484,    7,    7,  305, 6904,  138,   14,    1, 2438,    1,\n","       4515,  104,   70,   51,  245,    2,  340, 1554,   71,   11,   17,\n","         10,   77,   37,    1,  204,  126,   71,    1, 2145,  204,   18,\n","         11,   17,  624,    6,  126,   71,   29,    1,   97,  498, 4399,\n","          6,  138,    3,  668, 1051,   84,   28,    6,  809,   22,   98,\n","        816,    3, 5103,  219,   37,   87,  166,  183,  851,   83,   10,\n","        986,   11,   17,  514,   68, 4197,  100,  147,    9,   15,    3,\n","        171, 3096,    7,    7,  772,   78,   42,   11,   17,    6,   12,\n","          9,  149, 4881,   20,  601,   38,  460,    5, 2798,    1,  309,\n","          9,    6,   39, 1059, 2265,   44,   96,   74,    1,  305,  104,\n","          4,    1,  204,   36,   11,   27,   84,   11,    6,   14,   49,\n","          4,   32,  471,    4,    3, 2058, 1254,   17,   14,   33,  215])"]},"metadata":{},"execution_count":18}],"source":["# TODO: 정수로 변환된 0번째 텍스트만 출력해보기\n","# the는 1로, and는 2로, a는 3으로, of는 4로 변환됐는지 살펴보세요.\n","np.array(x_train_tokens[0])"]},{"cell_type":"markdown","metadata":{"id":"bvOL1jHAt2WU"},"source":["We also need to convert the texts in the test-set to tokens."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"FAjPUecQt2WU","executionInfo":{"status":"ok","timestamp":1651818489981,"user_tz":-540,"elapsed":3826,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["# TODO: tokenizer로 테스트 데이터의 텍스트를 정수로 변환해보세요.\n","x_test_tokens = tokenizer.texts_to_sequences(x_test_text)"]},{"cell_type":"markdown","metadata":{"id":"MHGyZPoEt2WU"},"source":["## Padding and Truncating Data\n","\n","순환신경망은 임의의 길이를 지닌 입력을 처리할 수 있지만 훈련의 속도를 높이기 위해 동일 길이를 지닌 입력 데이터의 묶음 (batch) 단위로 훈련이 이루어지는 관계로 편의상 모든 입력 데이터의 길이를 동일하게 합니다. 모든 입력 데이터의 길이를 동일하게 하는 방법은 크게 두 가지가 있습니다.\n","\n","1. 가장 길이가 긴 리뷰의 길이에 맞추어 다른 리뷰들의 앞, 혹은 뒤에 공백을 삽입한다.\n","\n","2. 적당한 길이를 정한다음 길이가 긴 리뷰는 앞, 혹은 뒤를 잘라내고 짧은 리뷰는 앞, 혹은 뒤에 공백을 삽입한다.\n","\n","첫 번째 방법은 지나치게 긴 리뷰가 있을 경우 다른 리뷰들에 지나치게 많은 공백이 들어가 데이터셋이 커져 훈련하는데 오래 걸리고 메모리의 낭비도 심해지는 단점이 있어 우리는 두 번째 방법을 사용합니다."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"RlDFg_w0t2WU","executionInfo":{"status":"ok","timestamp":1651818792165,"user_tz":-540,"elapsed":326,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["# 각 리뷰에 단어가 몇 개씩 들어있는지 세어봅시다.\n","\n","num_tokens = []\n","\n","for i in range(25000):\n","  num_tokens.append(len(x_train_tokens[i]))\n","\n","for i in range(25000):\n","  num_tokens.append(len(x_test_tokens[i]))\n","\n","# TODO: num_tokens에 단어 숫자를 저장하세요.\n","# num_tokens 원소는 50,000개이며 처음 25,000은 훈련 데이터의 단어숫자가 저장되어 있고,\n","# 나머지 25,000은 테스트 데이터의 단어숫자가 저장되어 있습니다.\n","# Hint: x_train_tokens[0]은 첫 번째 훈련 데이터의 단어들이 저장되어 있습니다. 그 길이는 len 함수로 알 수 있습니다.\n","\n","num_tokens = np.array(num_tokens) # TODO: 리스트를 넘파이 배열로 변환"]},{"cell_type":"markdown","metadata":{"id":"XYdxLK99t2WU"},"source":["The average number of tokens in a sequence is:"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"S0kuTohct2WV","outputId":"799dca74-d280-4762-f49c-de9c27e0f73e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651818795610,"user_tz":-540,"elapsed":332,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["221.27716"]},"metadata":{},"execution_count":30}],"source":["# 모든 리뷰에서 사용한 평균 단어의 수는 다음과 같습니다.\n","np.mean(num_tokens)"]},{"cell_type":"markdown","metadata":{"id":"lDu36sCDt2WV"},"source":["The maximum number of tokens in a sequence is:"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"IKujyc3It2WV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651818819872,"user_tz":-540,"elapsed":316,"user":{"displayName":"조현민","userId":"13701119480392953941"}},"outputId":"410d2f16-c18b-4da0-cd79-9514f079f9ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["2209\n"]}],"source":["# TODO: 가장 긴 리뷰의 단어수를 출력해보세요.\n","# Hint: np.max\n","print(np.max(num_tokens))"]},{"cell_type":"markdown","metadata":{"id":"CwAj_aOIt2WV"},"source":["The max number of tokens we will allow is set to the average plus 2 standard deviations."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"_M8wDkUJt2WV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651819011444,"user_tz":-540,"elapsed":328,"user":{"displayName":"조현민","userId":"13701119480392953941"}},"outputId":"2733b197-814d-406d-a134-819027f32d06"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["544"]},"metadata":{},"execution_count":32}],"source":["# TODO: num_tokens에서 평균 + 2 * 표준편차를 구하여 우리가 허용할 최대 단어의 개수인 max_tokens를 설정해보세요.\n","# num_tokens가 정규분포를 따른다면 97% 이상이 max_tokens보다 적은 단어만을 사용할 것입니다.\n","# Hint: np.mean과 np.std 사용\n","\n","max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n","max_tokens = int(max_tokens)\n","max_tokens"]},{"cell_type":"markdown","metadata":{"id":"B34-DJ_ot2WV"},"source":["This covers about 95% of the data-set."]},{"cell_type":"code","execution_count":33,"metadata":{"id":"Vg-lkIWjt2WV","outputId":"d1bc6703-6e3b-4cec-e78e-690ddfbfc457","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651819017985,"user_tz":-540,"elapsed":316,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.94534"]},"metadata":{},"execution_count":33}],"source":["# max_tokens보다 짧은 리뷰의 비율을 출력하는 코드\n","np.sum(num_tokens < max_tokens) / len(num_tokens)"]},{"cell_type":"markdown","source":["리뷰의 94% 이상은 max_tokens보다 짧으므로 공백을 삽입하여 길이를 max_tokens로 맞춰주고, max_tokens보다 긴 리뷰의 길이는 max_tokens로 자를 필요가 있습니다. 모든 리뷰를 가장 길었던 리뷰의 길이로 맞췄다면 데이터셋 크기가 굉장히 커졌을 것입니다.<br>\n","\n","공백을 삽입할 때 리뷰의 앞에 ('pre') 삽입할 수도 있고 마지막 ('post')에 삽입할 수도 있는데 인공신경망에 혼란을 주지 않기 위해 일관된 방식으로 삽입해야 합니다. (자르기도 동일)"],"metadata":{"id":"cX2jx6eINEJ_"}},{"cell_type":"code","execution_count":34,"metadata":{"id":"M8Y_JY2St2WW","executionInfo":{"status":"ok","timestamp":1651819035984,"user_tz":-540,"elapsed":320,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["# 공백을 리뷰의 앞에 삽입\n","pad = 'pre'"]},{"cell_type":"code","source":["max_tokens=584"],"metadata":{"id":"QXXf-mHeOnLB","executionInfo":{"status":"ok","timestamp":1651819037437,"user_tz":-540,"elapsed":1,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","execution_count":36,"metadata":{"id":"XkY-yQDgt2WW","executionInfo":{"status":"ok","timestamp":1651819039644,"user_tz":-540,"elapsed":685,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["# 훈련 데이터의 길이를 max_tokens로 맞춰주기\n","x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,\n","                            padding=pad, truncating=pad)"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"okwoNpgGt2WW","executionInfo":{"status":"ok","timestamp":1651819041206,"user_tz":-540,"elapsed":335,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["# TODO: 테스트 데이터의 길이를 max_tokens로 맞춰주세요.\n","x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,\n","                            padding=pad, truncating=pad)"]},{"cell_type":"markdown","metadata":{"id":"CCKTP6vPt2WW"},"source":["We have now transformed the training-set into one big matrix of integers (tokens) with this shape:"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"LVp4YkW7t2WW","outputId":"78f159d6-0e44-4f67-e953-7e1bc6a4de0b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651819042538,"user_tz":-540,"elapsed":8,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25000, 584)"]},"metadata":{},"execution_count":38}],"source":["# 훈련 데이터: (리뷰 개수, 각 리뷰의 길이)\n","x_train_pad.shape"]},{"cell_type":"markdown","metadata":{"id":"SrnD4H3Zt2WW"},"source":["The matrix for the test-set has the same shape:"]},{"cell_type":"code","execution_count":39,"metadata":{"scrolled":true,"id":"qJRJa6rgt2WW","outputId":"95ec09aa-07c9-4b55-ce6c-b3743c9ae8f2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651819044984,"user_tz":-540,"elapsed":450,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25000, 584)"]},"metadata":{},"execution_count":39}],"source":["# 테스트 데이터: (리뷰 개수, 각 리뷰의 길이)\n","x_test_pad.shape"]},{"cell_type":"markdown","metadata":{"id":"fzxij8nRt2WW"},"source":["For example, we had the following sequence of tokens above:"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"LjIbxctIt2WX","outputId":"7059abb1-d46d-4742-a588-d656b262fcfe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651819048396,"user_tz":-540,"elapsed":344,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  47, 1038,   43,   22,   23,    3,  191,  182,   17,  326,   91,\n","         22,   80,  118,   12, 2145,    1,   95,   15,  106, 1254,  104,\n","        403,  110,   11,   17,    6,    3,  289,  182,  360,    2,    6,\n","        406,   27,    4,    1, 6378,   97,  123,   90,   43,   21,    1,\n","       6378,    7,    7,   10,  160,  210,   11,   17,  100,  314,    1,\n","        370,    4,    1,  204,   89,  983,   68,  134,   10, 1228,  210,\n","          1,   79,  702,   97,  159,  314,   11,   27,  196,   39,   85,\n","          9,  949,   41,   18,   10,   58,   25,    5,  131,   11,   27,\n","       3303,   29,    1,  400,  243,    9,    6, 2039, 2611,    2,  314,\n","        498, 1560,   41,   36,  515,    3, 3599,   38, 1192,  183,    8,\n","          1,  457, 3249,    3, 7540,  175,  125, 6340,   56,  521,  173,\n","          4,    1,   17,   13,   50,  498, 9597,    3,  219,    2,   24,\n","        408,    5,   27,  486,    9,    6,   27,    4,    1, 1132,    8,\n","         17,  484,    7,    7,  305, 6904,  138,   14,    1, 2438,    1,\n","       4515,  104,   70,   51,  245,    2,  340, 1554,   71,   11,   17,\n","         10,   77,   37,    1,  204,  126,   71,    1, 2145,  204,   18,\n","         11,   17,  624,    6,  126,   71,   29,    1,   97,  498, 4399,\n","          6,  138,    3,  668, 1051,   84,   28,    6,  809,   22,   98,\n","        816,    3, 5103,  219,   37,   87,  166,  183,  851,   83,   10,\n","        986,   11,   17,  514,   68, 4197,  100,  147,    9,   15,    3,\n","        171, 3096,    7,    7,  772,   78,   42,   11,   17,    6,   12,\n","          9,  149, 4881,   20,  601,   38,  460,    5, 2798,    1,  309,\n","          9,    6,   39, 1059, 2265,   44,   96,   74,    1,  305,  104,\n","          4,    1,  204,   36,   11,   27,   84,   11,    6,   14,   49,\n","          4,   32,  471,    4,    3, 2058, 1254,   17,   14,   33,  215])"]},"metadata":{},"execution_count":40}],"source":["# 패딩 전의 0번째 훈련 데이터 \n","np.array(x_train_tokens[0])"]},{"cell_type":"markdown","metadata":{"id":"LVbARkK0t2WX"},"source":["This has simply been padded to create the following sequence. Note that when this is input to the Recurrent Neural Network, then it first inputs a lot of zeros. If we had padded 'post' then it would input the integer-tokens first and then a lot of zeros. This may confuse the Recurrent Neural Network."]},{"cell_type":"code","execution_count":41,"metadata":{"id":"WvblmFmht2WX","outputId":"3dfa329b-558a-4779-e405-7e728231fe76","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651819058040,"user_tz":-540,"elapsed":326,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,   47, 1038,   43,   22,   23,    3,  191,  182,   17,  326,\n","         91,   22,   80,  118,   12, 2145,    1,   95,   15,  106, 1254,\n","        104,  403,  110,   11,   17,    6,    3,  289,  182,  360,    2,\n","          6,  406,   27,    4,    1, 6378,   97,  123,   90,   43,   21,\n","          1, 6378,    7,    7,   10,  160,  210,   11,   17,  100,  314,\n","          1,  370,    4,    1,  204,   89,  983,   68,  134,   10, 1228,\n","        210,    1,   79,  702,   97,  159,  314,   11,   27,  196,   39,\n","         85,    9,  949,   41,   18,   10,   58,   25,    5,  131,   11,\n","         27, 3303,   29,    1,  400,  243,    9,    6, 2039, 2611,    2,\n","        314,  498, 1560,   41,   36,  515,    3, 3599,   38, 1192,  183,\n","          8,    1,  457, 3249,    3, 7540,  175,  125, 6340,   56,  521,\n","        173,    4,    1,   17,   13,   50,  498, 9597,    3,  219,    2,\n","         24,  408,    5,   27,  486,    9,    6,   27,    4,    1, 1132,\n","          8,   17,  484,    7,    7,  305, 6904,  138,   14,    1, 2438,\n","          1, 4515,  104,   70,   51,  245,    2,  340, 1554,   71,   11,\n","         17,   10,   77,   37,    1,  204,  126,   71,    1, 2145,  204,\n","         18,   11,   17,  624,    6,  126,   71,   29,    1,   97,  498,\n","       4399,    6,  138,    3,  668, 1051,   84,   28,    6,  809,   22,\n","         98,  816,    3, 5103,  219,   37,   87,  166,  183,  851,   83,\n","         10,  986,   11,   17,  514,   68, 4197,  100,  147,    9,   15,\n","          3,  171, 3096,    7,    7,  772,   78,   42,   11,   17,    6,\n","         12,    9,  149, 4881,   20,  601,   38,  460,    5, 2798,    1,\n","        309,    9,    6,   39, 1059, 2265,   44,   96,   74,    1,  305,\n","        104,    4,    1,  204,   36,   11,   27,   84,   11,    6,   14,\n","         49,    4,   32,  471,    4,    3, 2058, 1254,   17,   14,   33,\n","        215], dtype=int32)"]},"metadata":{},"execution_count":41}],"source":["# TODO: 패딩 후의 0번째 훈련 데이터를 출력해보세요.\n","np.array(x_train_pad[0])"]},{"cell_type":"markdown","metadata":{"id":"BuEuwTb7t2WX"},"source":["## Tokenizer Inverse Map\n","\n","정수의 배열을 다시 텍스트로 바꿔주는 함수를 만들어봅시다."]},{"cell_type":"code","execution_count":42,"metadata":{"id":"GcQFzRXkt2WX","executionInfo":{"status":"ok","timestamp":1651819213162,"user_tz":-540,"elapsed":337,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["idx = tokenizer.word_index # tokenizer는 단어 (key)-> 정수 (value)이므로 \n","inverse_map = dict(zip(idx.values(), idx.keys())) # 정수 (value) -> 단어 (key)로 보내면 됩니다."]},{"cell_type":"markdown","metadata":{"id":"mUsOFMx3t2WX"},"source":["Helper-function for converting a list of tokens back to a string of words."]},{"cell_type":"code","execution_count":43,"metadata":{"id":"CmNXSZX3t2WX","executionInfo":{"status":"ok","timestamp":1651819231947,"user_tz":-540,"elapsed":532,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["# tokens_to_string 함수에 정수 배열을 입력하면 텍스트로 출력됩니다.\n","def tokens_to_string(tokens):\n","    # Map from tokens back to words.\n","    words = [inverse_map[token] for token in tokens if token != 0]\n","    \n","    # Concatenate all words.\n","    text = \" \".join(words)\n","\n","    return text"]},{"cell_type":"markdown","metadata":{"id":"8joKACPJt2WX"},"source":["For example, this is the original text from the data-set:"]},{"cell_type":"code","execution_count":44,"metadata":{"scrolled":true,"id":"G7-Rex8ot2WX","outputId":"c01f3bb6-804e-4c3d-bfc9-eebeaaddf8b6","colab":{"base_uri":"https://localhost:8080/","height":167},"executionInfo":{"status":"ok","timestamp":1651819240841,"user_tz":-540,"elapsed":353,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Some spoilers If you are a big horror movie fan, then you will know that Halloween paved the way for many slasher films. Often imitated, never duplicated, this movie is a true horror classic and is definitely one of the scariest movies ever made, if not THE scariest.<br /><br />I actually saw this movie after seeing the rest of the series (don't ask me why). I honestly saw the other 7 movies before seeing this one, that's just how it worked out. But I would have to say this one blows all the others away. It is genuinely frightening, and seeing Michael pop out from behind a bush or walk around in the dark sends a chill down your spine. My favorite part of the movie was when Michael stabs a guy, and leans his head to one side. It is one of the eeriest images in movie history.<br /><br />Later slashers, such as the Friday the 13th films, were more fun and less intense than this movie. I do like the F13 series better than the Halloween series, but this movie alone is better than all the F13 movies. Michael Myers is such a scary villain because he is realistic, you could imagine a crazed guy like him going around killing people. I admit this movie gave me nightmares after watching it for a few nights.<br /><br />What's great about this movie is that it doesn't rely on gore or humor to entertain the audience. It is just pure terror. It's too bad the later films of the series swayed from this one, because this is as good of an example of a spectacular slasher movie as they come.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":44}],"source":["# 훈련 데이터의 0번째 리뷰 실제 텍스트\n","x_train_text[0]"]},{"cell_type":"code","source":["# 정수로 변환된 훈련 데이터의 0번째 리뷰\n","np.array(x_train_tokens[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1q-SfSwdQQS2","executionInfo":{"status":"ok","timestamp":1651819245019,"user_tz":-540,"elapsed":341,"user":{"displayName":"조현민","userId":"13701119480392953941"}},"outputId":"1c9dce72-f051-4fc6-c709-509f654efc0f"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  47, 1038,   43,   22,   23,    3,  191,  182,   17,  326,   91,\n","         22,   80,  118,   12, 2145,    1,   95,   15,  106, 1254,  104,\n","        403,  110,   11,   17,    6,    3,  289,  182,  360,    2,    6,\n","        406,   27,    4,    1, 6378,   97,  123,   90,   43,   21,    1,\n","       6378,    7,    7,   10,  160,  210,   11,   17,  100,  314,    1,\n","        370,    4,    1,  204,   89,  983,   68,  134,   10, 1228,  210,\n","          1,   79,  702,   97,  159,  314,   11,   27,  196,   39,   85,\n","          9,  949,   41,   18,   10,   58,   25,    5,  131,   11,   27,\n","       3303,   29,    1,  400,  243,    9,    6, 2039, 2611,    2,  314,\n","        498, 1560,   41,   36,  515,    3, 3599,   38, 1192,  183,    8,\n","          1,  457, 3249,    3, 7540,  175,  125, 6340,   56,  521,  173,\n","          4,    1,   17,   13,   50,  498, 9597,    3,  219,    2,   24,\n","        408,    5,   27,  486,    9,    6,   27,    4,    1, 1132,    8,\n","         17,  484,    7,    7,  305, 6904,  138,   14,    1, 2438,    1,\n","       4515,  104,   70,   51,  245,    2,  340, 1554,   71,   11,   17,\n","         10,   77,   37,    1,  204,  126,   71,    1, 2145,  204,   18,\n","         11,   17,  624,    6,  126,   71,   29,    1,   97,  498, 4399,\n","          6,  138,    3,  668, 1051,   84,   28,    6,  809,   22,   98,\n","        816,    3, 5103,  219,   37,   87,  166,  183,  851,   83,   10,\n","        986,   11,   17,  514,   68, 4197,  100,  147,    9,   15,    3,\n","        171, 3096,    7,    7,  772,   78,   42,   11,   17,    6,   12,\n","          9,  149, 4881,   20,  601,   38,  460,    5, 2798,    1,  309,\n","          9,    6,   39, 1059, 2265,   44,   96,   74,    1,  305,  104,\n","          4,    1,  204,   36,   11,   27,   84,   11,    6,   14,   49,\n","          4,   32,  471,    4,    3, 2058, 1254,   17,   14,   33,  215])"]},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"ya16yBQrt2WY"},"source":["We can recreate this text except for punctuation and other symbols, by converting the list of tokens back to words:"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"DqDwOJ9Lt2WY","outputId":"601427b3-24d5-4f44-ce9a-9d145e1b6731","colab":{"base_uri":"https://localhost:8080/","height":167},"executionInfo":{"status":"ok","timestamp":1651819346814,"user_tz":-540,"elapsed":348,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"some spoilers if you are a big horror movie fan then you will know that halloween the way for many slasher films often never this movie is a true horror classic and is definitely one of the scariest movies ever made if not the scariest br br i actually saw this movie after seeing the rest of the series don't ask me why i honestly saw the other 7 movies before seeing this one that's just how it worked out but i would have to say this one blows all the others away it is genuinely frightening and seeing michael pop out from behind a bush or walk around in the dark sends a chill down your spine my favorite part of the movie was when michael stabs a guy and his head to one side it is one of the images in movie history br br later slashers such as the friday the 13th films were more fun and less intense than this movie i do like the series better than the halloween series but this movie alone is better than all the movies michael myers is such a scary villain because he is realistic you could imagine a crazed guy like him going around killing people i admit this movie gave me nightmares after watching it for a few nights br br what's great about this movie is that it doesn't rely on gore or humor to entertain the audience it is just pure terror it's too bad the later films of the series from this one because this is as good of an example of a spectacular slasher movie as they come\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":46}],"source":["# TODO: 정수를 다시 단어로 변환해보세요.\n","# 모두 소문자로 변경되고, 잘 쓰이지 않는 단어들은 삭제된 상태입니다.\n","tokens_to_string(x_train_tokens[0])"]},{"cell_type":"markdown","metadata":{"id":"hamblZDft2WY"},"source":["## Create the Recurrent Neural Network\n","\n","We are now ready to create the Recurrent Neural Network (RNN). We will use the Keras API for this because of its simplicity. See Tutorial #03-C for a tutorial on Keras."]},{"cell_type":"code","execution_count":47,"metadata":{"id":"8nObzn4ft2WY","executionInfo":{"status":"ok","timestamp":1651819354976,"user_tz":-540,"elapsed":2907,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["model = Sequential()"]},{"cell_type":"markdown","metadata":{"id":"ggam4ai4t2WY"},"source":["자연어 처리를 위한 순환 신경망의 가장 첫 번째 층은 임베딩층입니다. 우리가 선택한 10000개의 단어를 우리가 지정한 차원에 보내 비슷한 단어가 가까운 곳에 위치하도록 만들 것입니다. (Tokenizer가 만든 정수는 1: the, 2: and, 3: a, 4: of과 같이 뜻이 비슷하지도 않은데 단어들이 가까운 곳에 위치해 있습니다.)\n","\n","일반적으로 임베딩 차원은 100에서 300 정도를 사용하는데 일단 8차원만 사용해봅시다. "]},{"cell_type":"code","execution_count":48,"metadata":{"id":"ObHSK_FKt2WY","executionInfo":{"status":"ok","timestamp":1651819740974,"user_tz":-540,"elapsed":358,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["embedding_size = 8"]},{"cell_type":"markdown","metadata":{"id":"OnOGpWR8t2WY"},"source":["The embedding-layer also needs to know the number of words in the vocabulary (`num_words`) and the length of the padded token-sequences (`max_tokens`). We also give this layer a name because we need to retrieve its weights further below."]},{"cell_type":"code","execution_count":49,"metadata":{"id":"sVBLbYdrt2WY","executionInfo":{"status":"ok","timestamp":1651819803526,"user_tz":-540,"elapsed":861,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["# 임베딩층 추가\n","model.add(Embedding(input_dim=num_words,      # 사용하는 단어의 개수\n","                    output_dim=embedding_size,# 임베딩 차원\n","                    input_length=max_tokens,  # 리뷰의 길이\n","                    name='layer_embedding'))"]},{"cell_type":"markdown","metadata":{"id":"36FqWbR9t2WY"},"source":["순환 유닛인 GRU를 세 층 쌓은 다음 Dense로 묶어 하나의 출력에서 0과 1사이의 실수를 출력하도록 합니다."]},{"cell_type":"code","execution_count":50,"metadata":{"id":"LnuPpp4zt2WY","executionInfo":{"status":"ok","timestamp":1651819826606,"user_tz":-540,"elapsed":1871,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["model.add(LSTM(units=16, return_sequences=True)) # 다음 층에 단어를 하나씩 처리하는 GRU가 있으므로 단어가 들어올 때마다 출력을 다음 층으로 전달\n","model.add(LSTM(units=8, return_sequences=True)) # 다음 층에 단어를 하나씩 처리하는 GRU가 있으므로 단어가 들어올 때마다 출력을 다음 층으로 전달\n","model.add(LSTM(units=4)) # 다음 층에 매 단어를 처리할 때마다 출력을 전달할 필요 없이 리뷰를 끝까지 읽고 나온 결과만 전달\n","model.add(Dense(1, activation='sigmoid')) # 이전 층의 입력을 받아들여 0부터 1사이의 실수값 하나 출력 (sigmoid)"]},{"cell_type":"markdown","metadata":{"id":"uUlN1Vxvt2WZ"},"source":["Use the Adam optimizer with the given learning-rate."]},{"cell_type":"code","execution_count":51,"metadata":{"id":"h9tJRHtZt2WZ","executionInfo":{"status":"ok","timestamp":1651819840859,"user_tz":-540,"elapsed":363,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["optimizer = Adam(learning_rate=1e-3)"]},{"cell_type":"markdown","metadata":{"id":"kah4uUD3t2WZ"},"source":["Compile the Keras model so it is ready for training."]},{"cell_type":"code","execution_count":52,"metadata":{"id":"KRiczqCyt2WZ","executionInfo":{"status":"ok","timestamp":1651819864065,"user_tz":-540,"elapsed":361,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["model.compile(loss='binary_crossentropy',\n","              optimizer=optimizer,\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"ZINq_6Bkt2Wa","outputId":"cf030374-df14-4885-8ca0-9d73b11c9da2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651819865352,"user_tz":-540,"elapsed":7,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," layer_embedding (Embedding)  (None, 584, 8)           80000     \n","                                                                 \n"," lstm (LSTM)                 (None, 584, 16)           1600      \n","                                                                 \n"," lstm_1 (LSTM)               (None, 584, 8)            800       \n","                                                                 \n"," lstm_2 (LSTM)               (None, 4)                 208       \n","                                                                 \n"," dense (Dense)               (None, 1)                 5         \n","                                                                 \n","=================================================================\n","Total params: 82,613\n","Trainable params: 82,613\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"xZYcJVRpt2Wa"},"source":["## Train the Recurrent Neural Network\n","\n","We can now train the model. Note that we are using the data-set with the padded sequences. We use 5% of the training-set as a small validation-set, so we have a rough idea whether the model is generalizing well or if it is perhaps over-fitting to the training-set."]},{"cell_type":"code","execution_count":54,"metadata":{"scrolled":true,"id":"tQxdKqGbt2Wa","outputId":"24430b11-0dd2-471b-f561-34c9c0e64f23","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651820184151,"user_tz":-540,"elapsed":313934,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","372/372 [==============================] - 112s 274ms/step - loss: 0.5429 - accuracy: 0.7207 - val_loss: 0.6907 - val_accuracy: 0.6976\n","Epoch 2/3\n","372/372 [==============================] - 101s 271ms/step - loss: 0.3493 - accuracy: 0.8639 - val_loss: 0.3644 - val_accuracy: 0.8480\n","Epoch 3/3\n","372/372 [==============================] - 101s 271ms/step - loss: 0.2709 - accuracy: 0.8986 - val_loss: 0.3429 - val_accuracy: 0.8648\n","CPU times: user 3min 27s, sys: 1min 35s, total: 5min 2s\n","Wall time: 5min 13s\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f5f65758510>"]},"metadata":{},"execution_count":54}],"source":["# validation을 통해 테스트셋에서의 성능을 가늠할 수 있습니다.\n","# 훈련데이터에 대한 성능은 개선되는데 validation에서 악화된다면 과적합이 일어나는 것일 수도 있습니다.\n","%%time\n","model.fit(x_train_pad, y_train,\n","          validation_split=0.05, epochs=3, batch_size=64)"]},{"cell_type":"markdown","metadata":{"id":"382P8ZGMt2Wa"},"source":["## Performance on Test-Set\n","\n","Now that the model has been trained we can calculate its classification accuracy on the test-set."]},{"cell_type":"code","execution_count":55,"metadata":{"id":"hP1eEs8xt2Wa","outputId":"398cd644-ef9b-4e36-d069-a20c3210cf03","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651820300306,"user_tz":-540,"elapsed":86802,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["782/782 [==============================] - 86s 110ms/step - loss: 0.3611 - accuracy: 0.8500\n","CPU times: user 50.5 s, sys: 28.1 s, total: 1min 18s\n","Wall time: 1min 26s\n"]}],"source":["# 테스트셋에서의 성능 평가\n","%%time\n","result = model.evaluate(x_test_pad, y_test)"]},{"cell_type":"code","execution_count":56,"metadata":{"scrolled":true,"id":"McKlnypZt2Wa","outputId":"a8cee702-8bf7-4677-c8b4-2b66e436dc1a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651820303474,"user_tz":-540,"elapsed":296,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 85.00%\n"]}],"source":["print(\"Accuracy: {0:.2%}\".format(result[1]))"]},{"cell_type":"markdown","metadata":{"id":"alpR4ZXut2Wc"},"source":["## New Data\n","\n","Let us try and classify new texts that we make up. Some of these are obvious, while others use negation and sarcasm to try and confuse the model into mis-classifying the text."]},{"cell_type":"code","execution_count":57,"metadata":{"id":"7dguFHY5t2Wc","executionInfo":{"status":"ok","timestamp":1651820311772,"user_tz":-540,"elapsed":1143,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["# 새로운 데이터셋 8개 생성하여 texts에 저장\n","text1 = \"This movie is fantastic! I really like it because it is so good!\"\n","text2 = \"Good movie!\"\n","text3 = \"Maybe I like this movie.\"\n","text4 = \"Meh ...\"\n","text5 = \"If I were a drunk teenager then this movie might be good.\"\n","text6 = \"Bad movie!\"\n","text7 = \"Not a good movie!\"\n","text8 = \"This movie really sucks! Can I get my money back please?\"\n","texts = [text1, text2, text3, text4, text5, text6, text7, text8]"]},{"cell_type":"markdown","metadata":{"id":"L21c2SlFt2Wc"},"source":["We first convert these texts to arrays of integer-tokens because that is needed by the model."]},{"cell_type":"code","execution_count":58,"metadata":{"id":"N53OFtJxt2Wd","executionInfo":{"status":"ok","timestamp":1651820318224,"user_tz":-540,"elapsed":312,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["# TODO: tokenizer로 texts를 정수로 변환하세요.\n","tokens = tokenizer.texts_to_sequences(texts)"]},{"cell_type":"markdown","metadata":{"id":"9tX0cSgCt2Wd"},"source":["To input texts with different lengths into the model, we also need to pad and truncate them."]},{"cell_type":"code","execution_count":59,"metadata":{"id":"gCZytuX-t2Wd","outputId":"703296f6-ff84-4dc7-d17f-fc1431bb92fc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651820346430,"user_tz":-540,"elapsed":318,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8, 584)"]},"metadata":{},"execution_count":59}],"source":["# TODO: 패딩으로 tokens의 길이를 맞춰주세요.\n","tokens_pad = pad_sequences(tokens, maxlen=max_tokens,\n","                           padding=pad, truncating=pad)\n","tokens_pad.shape"]},{"cell_type":"markdown","metadata":{"id":"TPwl4x3nt2Wd"},"source":["We can now use the trained model to predict the sentiment for these texts."]},{"cell_type":"code","execution_count":63,"metadata":{"id":"5smpP0MOt2Wd","outputId":"a364f03b-a396-4196-ea27-a7b479ab53fa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651820546520,"user_tz":-540,"elapsed":310,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.2777435 ],\n","       [0.0509877 ],\n","       [0.04963561],\n","       [0.04945935],\n","       [0.05264492],\n","       [0.04582264],\n","       [0.04988291],\n","       [0.0435621 ]], dtype=float32)"]},"metadata":{},"execution_count":63}],"source":["# TODO: model로 tokens_pad를 예측해보세요.\n","model.predict(tokens_pad)\n","# 1에 가까울 수록 긍정적이고 0에 가까울 수록 부정적입니다.\n","# 잘 예측했나요? 운에 따라 결과가 다를 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"xYhK-DEot2Wd"},"source":["A value close to 0.0 means a negative sentiment and a value close to 1.0 means a positive sentiment. These numbers will vary every time you train the model."]},{"cell_type":"markdown","metadata":{"id":"THYTOZSet2Wd"},"source":["## Embeddings\n","\n","임베딩을 통해 각 단어가 어떻게 변환되는지 살펴봅시다.\n","\n","The model cannot work on integer-tokens directly, because they are integer values that may range between 0 and the number of words in our vocabulary, e.g. 10000. So we need to convert the integer-tokens into vectors of values that are roughly between -1.0 and 1.0 which can be used as input to a neural network.\n","\n","This mapping from integer-tokens to real-valued vectors is also called an \"embedding\". It is essentially just a matrix where each row contains the vector-mapping of a single token. This means we can quickly lookup the mapping of each integer-token by simply using the token as an index into the matrix. The embeddings are learned along with the rest of the model during training.\n","\n","Ideally the embedding would learn a mapping where words that are similar in meaning also have similar embedding-values. Let us investigate if that has happened here.\n","\n","First we need to get the embedding-layer from the model:"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"jtyAVJbpt2Wd","executionInfo":{"status":"ok","timestamp":1651820552261,"user_tz":-540,"elapsed":323,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["# 임베딩 층 불러오기\n","layer_embedding = model.get_layer('layer_embedding')"]},{"cell_type":"markdown","metadata":{"id":"9jANfbGKt2Wd"},"source":["We can then get the weights used for the mapping done by the embedding-layer."]},{"cell_type":"code","execution_count":65,"metadata":{"id":"uVO-XSXrt2We","executionInfo":{"status":"ok","timestamp":1651820554748,"user_tz":-540,"elapsed":299,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["# 임베딩 층의 가중치 불러오기\n","weights_embedding = layer_embedding.get_weights()[0]"]},{"cell_type":"markdown","metadata":{"id":"tO6qnaB0t2We"},"source":["Note that the weights are actually just a matrix with the number of words in the vocabulary times the vector length for each embedding. That's because it is basically just a lookup-matrix."]},{"cell_type":"code","execution_count":66,"metadata":{"id":"-YcE3C_kt2We","outputId":"48d18885-ba23-4a86-f2df-988c935a5407","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651820557627,"user_tz":-540,"elapsed":329,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10000, 8)"]},"metadata":{},"execution_count":66}],"source":["# 10,000개의 단어를 8차원으로 보내고 있습니다.\n","weights_embedding.shape"]},{"cell_type":"markdown","metadata":{"id":"LwqGhFLVt2We"},"source":["Let us get the integer-token for the word 'good', which is just an index into the vocabulary."]},{"cell_type":"code","execution_count":67,"metadata":{"id":"M9xEoo2Ht2We","outputId":"744ca37c-dc4c-46a5-c999-09e2c982d235","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651820567164,"user_tz":-540,"elapsed":303,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["49"]},"metadata":{},"execution_count":67}],"source":["# tokenizer에서 good의 위치\n","token_good = tokenizer.word_index['good']\n","token_good"]},{"cell_type":"markdown","metadata":{"id":"ec1XR9Gut2We"},"source":["Let us also get the integer-token for the word 'great'."]},{"cell_type":"code","execution_count":68,"metadata":{"id":"fFz5EWFPt2We","outputId":"cb6c7828-602a-4b62-896b-5f9f44c8c168","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651820571986,"user_tz":-540,"elapsed":328,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["78"]},"metadata":{},"execution_count":68}],"source":["# tokenizer에서 great의 위치\n","token_great = tokenizer.word_index['great']\n","token_great"]},{"cell_type":"markdown","metadata":{"id":"1itM760At2Wf"},"source":["These integertokens may be far apart and will depend on the frequency of those words in the data-set.\n","\n","Now let us compare the vector-embeddings for the words 'good' and 'great'. Several of these values are similar, although some values are quite different. Note that these values will change every time you train the model."]},{"cell_type":"code","execution_count":69,"metadata":{"id":"Ju_7MpFrt2Wf","outputId":"dedd4315-1b1e-4d7d-f5fc-c9df30237b7d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651820576959,"user_tz":-540,"elapsed":314,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-0.06196402, -0.05886203,  0.03255121, -0.0675082 , -0.03757516,\n","        0.00383595, -0.09604653, -0.01444049], dtype=float32)"]},"metadata":{},"execution_count":69}],"source":["# good의 임베딩 결과\n","weights_embedding[token_good]"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"K4aw3__At2Wf","outputId":"b6215bf0-e1ee-4372-dc15-17cc218848ed","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651820587651,"user_tz":-540,"elapsed":307,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-0.12897176, -0.1709173 ,  0.10064013, -0.14114043, -0.10351828,\n","        0.12144257, -0.16843387, -0.15458387], dtype=float32)"]},"metadata":{},"execution_count":70}],"source":["# great의 임베딩 결과\n","weights_embedding[token_great]"]},{"cell_type":"markdown","metadata":{"id":"VrbHzmFlt2Wf"},"source":["Similarly, we can compare the embeddings for the words 'bad' and 'horrible'."]},{"cell_type":"code","execution_count":71,"metadata":{"id":"WYOkp0DIt2Wf","executionInfo":{"status":"ok","timestamp":1651820607577,"user_tz":-540,"elapsed":292,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["# tokenizer에서 bad와 horrible\n","token_bad = tokenizer.word_index['bad']\n","token_horrible = tokenizer.word_index['horrible']"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"DhfbcEnit2Wf","outputId":"757645ca-afac-41cc-8552-90a9e28b27ec","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651820608941,"user_tz":-540,"elapsed":2,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.16504468,  0.1320412 , -0.16460104,  0.15065904,  0.14640267,\n","       -0.1790877 ,  0.1650315 ,  0.16550215], dtype=float32)"]},"metadata":{},"execution_count":72}],"source":["# bad의 임베딩 결과\n","weights_embedding[token_bad]"]},{"cell_type":"code","execution_count":73,"metadata":{"id":"8uqa8Krzt2Wf","outputId":"955123c3-7c92-443d-f9fc-b76490792cdc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651820611695,"user_tz":-540,"elapsed":337,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.22962037,  0.25204763, -0.23706368,  0.22581927,  0.23408735,\n","       -0.25587776,  0.25060368,  0.15300325], dtype=float32)"]},"metadata":{},"execution_count":73}],"source":["# horrible의 임베딩 결과\n","weights_embedding[token_horrible]"]},{"cell_type":"markdown","source":["임베딩 결과를 보면 good과 great은 비슷한 곳에 위치하고 good과 bad는 반대되는 곳에 위치한 것을 볼 수 있습니다."],"metadata":{"id":"0zOsin_OXnsU"}},{"cell_type":"markdown","metadata":{"id":"6gDSIg26t2Wf"},"source":["### Sorted Words\n","\n","코사인 유사도로 임베딩 공간에서 비슷한 단어를 찾아봅시다.\n"]},{"cell_type":"code","execution_count":74,"metadata":{"id":"YyhM3RL0t2Wf","executionInfo":{"status":"ok","timestamp":1651820623579,"user_tz":-540,"elapsed":333,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[],"source":["# 주어진 단어와 비슷한 단어를 찾아주는 함수\n","def print_sorted_words(word, metric='cosine'):\n","    \"\"\"\n","    Print the words in the vocabulary sorted according to their\n","    embedding-distance to the given word.\n","    Different metrics can be used, e.g. 'cosine' or 'euclidean'.\n","    \"\"\"\n","\n","    # Get the token (i.e. integer ID) for the given word.\n","    token = tokenizer.word_index[word]\n","\n","    # Get the embedding for the given word. Note that the\n","    # embedding-weight-matrix is indexed by the word-tokens\n","    # which are integer IDs.\n","    embedding = weights_embedding[token]\n","\n","    # Calculate the distance between the embeddings for\n","    # this word and all other words in the vocabulary.\n","    distances = cdist(weights_embedding, [embedding],\n","                      metric=metric).T[0]\n","    \n","    # Get an index sorted according to the embedding-distances.\n","    # These are the tokens (integer IDs) for words in the vocabulary.\n","    sorted_index = np.argsort(distances)\n","    \n","    # Sort the embedding-distances.\n","    sorted_distances = distances[sorted_index]\n","    \n","    # Sort all the words in the vocabulary according to their\n","    # embedding-distance. This is a bit excessive because we\n","    # will only print the top and bottom words.\n","    sorted_words = [inverse_map[token] for token in sorted_index\n","                    if token != 0]\n","\n","    # Helper-function for printing words and embedding-distances.\n","    def _print_words(words, distances):\n","        for word, distance in zip(words, distances):\n","            print(\"{0:.3f} - {1}\".format(distance, word))\n","\n","    # Number of words to print from the top and bottom of the list.\n","    k = 10\n","\n","    print(\"Distance from '{0}':\".format(word))\n","\n","    # Print the words with smallest embedding-distance.\n","    _print_words(sorted_words[0:k], sorted_distances[0:k])\n","\n","    print(\"...\")\n","\n","    # Print the words with highest embedding-distance.\n","    _print_words(sorted_words[-k:], sorted_distances[-k:])"]},{"cell_type":"markdown","metadata":{"id":"ewbOIavTt2Wg"},"source":["We can then print the words that are near and far from the word 'great' in terms of their vector-embeddings. Note that these may change each time you train the model."]},{"cell_type":"code","execution_count":75,"metadata":{"scrolled":true,"id":"CFU1vStzt2Wg","outputId":"d21b055c-4247-4511-995d-da6aaf39268e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651820634823,"user_tz":-540,"elapsed":346,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Distance from 'great':\n","0.000 - great\n","0.003 - debra\n","0.007 - noah\n","0.007 - freedom\n","0.007 - marvelous\n","0.007 - bittersweet\n","0.008 - tight\n","0.009 - integrity\n","0.009 - graduation\n","0.010 - advanced\n","...\n","1.988 - motions\n","1.989 - slugs\n","1.989 - ridiculous\n","1.990 - insulting\n","1.992 - unfunny\n","1.992 - tedious\n","1.993 - terrible\n","1.993 - sucks\n","1.994 - unless\n","1.998 - sucked\n"]}],"source":["# great와 비슷한 단어 10개, 반대 단어 10개\n","print_sorted_words('great', metric='cosine')"]},{"cell_type":"markdown","metadata":{"id":"0tZi34Trt2Wg"},"source":["Similarly, we can print the words that are near and far from the word 'worst' in terms of their vector-embeddings."]},{"cell_type":"code","execution_count":76,"metadata":{"scrolled":true,"id":"P3qmBJ95t2Wg","outputId":"94c4ef89-e51c-4bb5-e8f2-feace8671701","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651820647319,"user_tz":-540,"elapsed":323,"user":{"displayName":"조현민","userId":"13701119480392953941"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Distance from 'worst':\n","0.000 - worst\n","0.005 - save\n","0.005 - bland\n","0.005 - lacks\n","0.006 - pathetic\n","0.006 - unnecessary\n","0.006 - thunderbirds\n","0.007 - thousands\n","0.007 - mundane\n","0.008 - advice\n","...\n","1.991 - integrity\n","1.992 - refreshing\n","1.992 - innovative\n","1.993 - mathieu\n","1.994 - perfect\n","1.995 - surpasses\n","1.996 - sheridan\n","1.996 - excellent\n","1.997 - vincenzo\n","1.997 - cry\n"]}],"source":["# worst와 비슷한 단어 10개, 반대 단어 10개\n","print_sorted_words('worst', metric='cosine')"]},{"cell_type":"markdown","metadata":{"id":"ZdkL2Jubt2Wg"},"source":["## Conclusion\n","\n","감성 분석에서 순환 신경망은 비교적 만족스러운 성능을 보이지만 사람과는 전혀 다른 방식으로 감정을 계산해냅니다. "]},{"cell_type":"markdown","metadata":{"id":"wnclb77Ct2Wg"},"source":["## 연습문제\n","\n","* 현재 Tokenizer가 가장 자주 쓰이는 10,000개의 단어를 처리하는데 5000개만 사용하면 성능이 어떻게 될까요? \n","* 임베딩을 8차원에서 수행하는데 200차원으로 늘리면 성능이 어떻게 변할까요?\n","* 패딩 옵션을 'pre'에서 'post'로 변경하면 성능이 어떻게 변할까요?\n"]},{"cell_type":"code","source":["# 테스트 셋 정확도\n","# 기존 : Accuracy: 85.00%\n","\n","# 10000 -> 5000 \n","# Accuracy: 86.66% 증가\n","\n","# 8 -> 200\n","# Accuracy: 84.96% 감소\n","\n","# pre -> post\n","# Accuracy: 50.14% 감소\n","\n","# 셋 다 수행 \n","# Accuracy: 50.70% 감소"],"metadata":{"id":"CwaH2-1cLxIE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NTjDZ271t2Wg"},"source":["## License (MIT)\n","\n","Copyright (c) 2022 by uramoon@kw.ac.kr<br>\n","Copyright (c) 2018 by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n","\n","The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n","\n","THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"KW_DLLAB_Natural_Language_Processing(조현민).ipynb","provenance":[{"file_id":"1SzeAqcsDFJ2VvDdhB7C549NsaEjtQ8U4","timestamp":1651816875259},{"file_id":"1MI86l3ewshqvZQsLXbu6OJfdVSnPwOzd","timestamp":1651673351961},{"file_id":"14xnkhvsEIycaSin1nwg_3lUkA0oQtMhr","timestamp":1651583420267},{"file_id":"https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/20_Natural_Language_Processing.ipynb","timestamp":1651578651328}],"collapsed_sections":["ZdkL2Jubt2Wg","NTjDZ271t2Wg"]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}