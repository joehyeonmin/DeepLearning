{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KW_MMDS- Colab 2.ipynb의 사본의 사본","provenance":[{"file_id":"1vs3wAcjr4MP3qcmfCNjCCSHLrVc5yZDu","timestamp":1646907793365},{"file_id":"1jgVf1cwl5X78nQxVuXEH0FLjyjpdCqwe","timestamp":1633495185691},{"file_id":"196Ot5E7T-u91vf1wBqwGu68IEDnLHgFG","timestamp":1601737874630},{"file_id":"1Db3fj_3jlDXgdrFWH6fUe03QF6ZT5EL5","timestamp":1600063016702}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kPt5q27L5557"},"source":["# KW_MMDS - Colab 2\n","## Frequent Pattern Mining in Spark"]},{"cell_type":"markdown","metadata":{"id":"p0-YhEpP_Ds-"},"source":["### Setup"]},{"cell_type":"markdown","metadata":{"id":"Zsj5WYpR9QId"},"source":["Let's setup Spark on your Colab environment.  Run the cell below!"]},{"cell_type":"code","metadata":{"id":"k-qHai2252mI"},"source":["!pip install pyspark\n","!pip install -U -q PyDrive\n","!apt install openjdk-8-jdk-headless -qq\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-CJ71AKe91eh"},"source":["Now we authenticate a Google Drive client to download the file we will be processing in our Spark job.\n","\n","**Make sure to follow the interactive instructions.**"]},{"cell_type":"code","metadata":{"id":"5K93ABEy9Zlo"},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0orRvrc1-545"},"source":["id='1dhi1F78ssqR8gE6U-AgB80ZW7V_9snX4'\n","downloaded = drive.CreateFile({'id': id})\n","downloaded.GetContentFile('products.csv')\n","\n","id='1KZBNEaIyMTcsRV817us6uLZgm-Mii8oU'\n","downloaded = drive.CreateFile({'id': id})\n","downloaded.GetContentFile('order_products__train.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qwtlO4_m_LbQ"},"source":["If you executed the cells above, you should be able to see the dataset we will need for this Colab under the \"Files\" tab on the left panel."]},{"cell_type":"code","metadata":{"id":"twk-K-jilWK7"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import pyspark\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dr-8fK-1lmY0"},"source":["Let's initialize the Spark context."]},{"cell_type":"code","metadata":{"id":"UOwtm2l7lePt"},"source":["# create the session\n","conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n","\n","# create the context\n","sc = pyspark.SparkContext(conf=conf)\n","spark = SparkSession.builder.getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CRaF2A_j_nC7"},"source":["### Your task"]},{"cell_type":"markdown","metadata":{"id":"ebLNUxP0_8x3"},"source":["If you run successfully the setup stage, you are ready to work with the **3 Million Instacart Orders** dataset. In case you want to read more about it, check the [official Instacart blog post](https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2) about it, a concise [schema description](https://gist.github.com/jeremystan/c3b39d947d9b88b3ccff3147dbcf6c6b) of the dataset, and the [download page](https://www.instacart.com/datasets/grocery-shopping-2017).\n","\n","In this Colab, we will be working only with a small training dataset (~131K orders) to perform fast Frequent Pattern Mining with the FP-Growth algorithm."]},{"cell_type":"code","metadata":{"id":"xu-e7Ph2_ruG"},"source":["products = spark.read.csv('products.csv', header=True, inferSchema=True)\n","orders = spark.read.csv('order_products__train.csv', header=True, inferSchema=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hhxZZRT9syUO","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1601724504660,"user_tz":-540,"elapsed":444,"user":{"displayName":"Seunghyun Moon","photoUrl":"https://lh6.googleusercontent.com/-Tf_qP8TpFZY/AAAAAAAAAAI/AAAAAAAAAXI/K-je7cpYEHc/s64/photo.jpg","userId":"02032424305378105022"}},"outputId":"2d644ef6-bc8a-4e7e-ac7a-686070294465"},"source":["products.printSchema()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["root\n"," |-- product_id: integer (nullable = true)\n"," |-- product_name: string (nullable = true)\n"," |-- aisle_id: string (nullable = true)\n"," |-- department_id: string (nullable = true)\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8VeRYRz2s1pm","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1601724506571,"user_tz":-540,"elapsed":639,"user":{"displayName":"Seunghyun Moon","photoUrl":"https://lh6.googleusercontent.com/-Tf_qP8TpFZY/AAAAAAAAAAI/AAAAAAAAAXI/K-je7cpYEHc/s64/photo.jpg","userId":"02032424305378105022"}},"outputId":"c4c6e64f-d950-49da-b6e4-78de2ea06f72"},"source":["orders.printSchema()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["root\n"," |-- order_id: integer (nullable = true)\n"," |-- product_id: integer (nullable = true)\n"," |-- add_to_cart_order: integer (nullable = true)\n"," |-- reordered: integer (nullable = true)\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h5muD_Io59CG"},"source":["Use the Spark Dataframe API to join 'products' and 'orders', so that you will be able to see the product names in each transaction (and not only their ids).  Then, group by the orders by 'order_id' to obtain one row per basket (i.e., set of products purchased together by one customer). "]},{"cell_type":"markdown","metadata":{"id":"EfHoTLAg6qnM"},"source":["In this Colab we will explore [MLlib](https://spark.apache.org/mllib/), Apache Spark's scalable machine learning library. Specifically, you can use its implementation of the [FP-Growth](https://spark.apache.org/docs/latest/ml-frequent-pattern-mining.html#fp-growth) algorithm to perform efficiently Frequent Pattern Mining in Spark.\n","Use the Python example in the documentation, and train a model with \n","\n","```minSupport=0.01``` and ```minConfidence=0.5```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6kpTVdfD8UiO"},"source":["1. Compute how many frequent itemsets and association rules were generated by running FP-growth.\n"]},{"cell_type":"code","metadata":{"id":"6KYgQ_URunvA"},"source":["# YOUR CODE HERE\n","orders_joined = orders.join(products, orders.product_id == products.product_id)\n","orders_joined = orders_joined.selectExpr([\"order_id\", \"product_name\"])\n","# orders_joined.show(truncate=False)\n","\n","# order_id로 묶으면서 product_name들을 모아 items라고 이름을 붙임\n","df = orders_joined.groupBy('order_id').agg(collect_set('product_name').alias('items'))\n","# df.show(truncate=False)\n","\n","from pyspark.ml.fpm import FPGrowth\n","fpGrowth = FPGrowth(itemsCol = \"items\", minSupport=0.01, minConfidence=0.5)\n","model = fpGrowth.fit(df)\n","\n","# Display frequent itemsets.\n","# model.freqItemsets.show()\n","print(model.freqItemsets.count())\n","\n","# Display generated association rules.\n","# model.associationRules.show()\n","print(model.associationRules.count())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oe-P_Fciz2mi"},"source":["2. What is the most frequent item?"]},{"cell_type":"code","metadata":{"id":"chK7L-tegqYm"},"source":["# YOUR CODE HERE\n","model.freqItemsets.sort('freq', ascending=False).show(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qT8Lwm1VAPoN"},"source":["3. Now retrain the FP-growth model changing only \n","```minsupport=0.001``` \n","and compute how many frequent itemsets and association rules were generated.\n"]},{"cell_type":"code","metadata":{"id":"F5Fa9spFz_HY"},"source":["# YOUR CODE HERE\n","fpGrowth = FPGrowth(itemsCol = \"items\", minSupport=0.001, minConfidence=0.5)\n","model = fpGrowth.fit(df)\n","\n","# Display frequent itemsets.\n","print(model.freqItemsets.count())\n","\n","# Display generated association rules.\n","print(model.associationRules.count())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5kZSknUjei0u"},"source":["4. Print all the association rules of the above problem."]},{"cell_type":"code","metadata":{"id":"UenciqLXgtZD"},"source":["# YOUR CODE HERE\n","model.associationRules.show(truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P-2tb-BegvbC"},"source":["5. What can be inferred from the association rules? Write a comment you can get from the rules."]},{"cell_type":"code","metadata":{"id":"6qvHiCowh2mc"},"source":["# YOUR TEXT HERE\n","# (IN ENGLISH OR KOREAN)\n","유기농 제품을 선호하는 사람은 대부분의 제품을 유기농 제품으로 구매한다.\n","매장에 진열할 때 유기농 제품끼리는 가까운 곳에 진열하면 좋을 것 같다.\n","\n","양파는 요리 대부분에 사용되기 때문에 사람들이 대부분 장을 볼때마다 사는 야채이다.\n","그리고 딸기는 다른 과일 대비 가격이 비싸고 양이 적기 때문에 사계절 내내 먹을 수 있고 \n","상대적으로 가격 대비 양이 많은 바나나를 같이 구입하는 경향이 있는 것 같다. \n","고가의 과일 주위에 바나나 매대를 설치하면 좋을 것 같다."],"execution_count":null,"outputs":[]}]}