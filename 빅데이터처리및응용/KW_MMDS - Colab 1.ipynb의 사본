{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KW_MMDS - Colab 1.ipynb의 사본","provenance":[{"file_id":"1I29N2KxSZO0NZX2Tvnvz0zSBAxv_klAr","timestamp":1646907588297},{"file_id":"1OYY1n6iSrpP7ET5H2PUchl6xEMnC1boD","timestamp":1600668811736}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kPt5q27L5557"},"source":["# KW_MMDS - Colab 1\n","## Wordcount in Spark"]},{"cell_type":"markdown","metadata":{"id":"p0-YhEpP_Ds-"},"source":["### Setup"]},{"cell_type":"markdown","metadata":{"id":"Zsj5WYpR9QId"},"source":["Let's setup Spark on your Colab environment.  Run the cell below!"]},{"cell_type":"code","metadata":{"id":"k-qHai2252mI"},"source":["!pip install pyspark\n","!pip install -U -q PyDrive\n","!apt install openjdk-8-jdk-headless -qq\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-CJ71AKe91eh"},"source":["Now we authenticate a Google Drive client to download the file we will be processing in our Spark job.\n","\n","**Make sure to follow the interactive instructions.**"]},{"cell_type":"code","metadata":{"id":"5K93ABEy9Zlo"},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0orRvrc1-545"},"source":["id='1SE6k_0YukzGd5wK-E4i6mG83nydlfvSa'\n","downloaded = drive.CreateFile({'id': id})\n","downloaded.GetContentFile('pg100.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qwtlO4_m_LbQ"},"source":["If you executed the cells above, you should be able to see the file *pg100.txt* under the \"Files\" tab on the left panel."]},{"cell_type":"markdown","metadata":{"id":"CRaF2A_j_nC7"},"source":["### Your task"]},{"cell_type":"markdown","metadata":{"id":"ebLNUxP0_8x3"},"source":["If you run successfully the setup stage, you are ready to work on the *pg100.txt* file which contains a copy of the complete works of Shakespeare.\n","\n","1. Write a Spark application which outputs the number of words that start with each letter. This means that for every letter we want to count the total number of (non-unique) words that start with a specific letter. In your implementation **ignore the letter case**, i.e., consider all words as lower case. Also, you can ignore all the words **starting** with a non-alphabetic character."]},{"cell_type":"code","metadata":{"id":"xu-e7Ph2_ruG"},"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext\n","import pandas as pd\n","\n","# create the Spark Session\n","spark = SparkSession.builder.getOrCreate()\n","\n","# create the Spark Context\n","sc = spark.sparkContext"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AuAxGFPFB43Y"},"source":["# YOUR CODE HERE, USE THIS CELL ONLY (DO NOT USE ADDITIONAL CELLS.)\n","RDDs = sc.textFile('pg100.txt').map(lambda line: line.lower())\\\n","\n","                              .flatMap(lambda line: line.split(\" \"))\\\n","\n","                              .filter(lambda word: len(word) > 0)\\\n","\n","                              .filter(lambda word: word[0].isalpha())\\\n","\n","                              .map(lambda word: (word[0], 1))\\\n","\n","                              .reduceByKey(lambda a, b: a + b)\\\n","\n","                              .sortByKey()                             \n","\n","\n","\n","RDDs.take(26)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FTuW5Ib5TUam"},"source":["2. Now, you have to work on the *1342-0.txt* file which contains 'the Pride and Prejudice' by Jane Austen. Write a Spark application which outputs the 10 most frequently used words that start with **uppercase**. "]},{"cell_type":"code","metadata":{"id":"HMVQwOIlYPnr"},"source":["# 오만과 편견 다운로드: 1342-0.txt\n","pride_id='11HHiWgRiXHuNehW8AXrHarKJfGX1HHnB'\n","pride_downloaded = drive.CreateFile({'id': pride_id})\n","pride_downloaded.GetContentFile('1342-0.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ONa40SSMdbCw"},"source":["# YOUR CODE HERE, USE THIS CELL ONLY (DO NOT USE ADDITIONAL CELLS.)\n","RDDs = sc.textFile('1342-0.txt').flatMap(lambda line: line.split(\" \"))\\\n","                              .filter(lambda word: len(word) > 0)\\\n","                              .filter(lambda word: word[0].isupper())\\\n","                              .map(lambda word: (word, 1))\\\n","                              .reduceByKey(lambda a, b: a + b)\\\n","                              .map(lambda tuple: (tuple[1], tuple[0]))\\\n","                              .sortByKey(ascending=False)\n","RDDs.take(10)"],"execution_count":null,"outputs":[]}]}